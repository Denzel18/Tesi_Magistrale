{"cells":[{"cell_type":"markdown","metadata":{"id":"l1Fs0GL_mVgn"},"source":["# ***TensorFlow - Adversarial Architecture ***"]},{"cell_type":"markdown","metadata":{"id":"ox6IFmrBfpP-"},"source":["## IMPORT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8298,"status":"ok","timestamp":1635800284785,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"mwFSXnnkfot7","outputId":"1c5b340e-d852-4189-c431-d83777a293cc"},"outputs":[],"source":["'''IMPORTING LIBRARIES'''\n","\n","'''Import packages &libraries for all the rest of the code'''\n","import sys\n","import subprocess\n","if 'google.colab' in sys.modules:\n","  subprocess.call(\"pip install -U progress\".split())\n","\n","import pandas as pd\n","import random\n","import os\n","import scipy.ndimage\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import sklearn as sk\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras import optimizers, layers\n","from tensorflow.keras.layers import Activation, Input, Conv2D, ZeroPadding2D, MaxPooling2D, UpSampling2D, concatenate, Flatten, Dense, Dropout\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.utils import to_categorical \n","from PIL import Image, ImageOps\n","\n","from google.colab.patches import cv2_imshow\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.models import Sequential, save_model, load_model\n","import matplotlib.pyplot as plt\n","import cv2\n","from random import randrange\n","random.seed( 40 )\n","\n","print('Tensor Flow {}'.format(tf.__version__))\n","print('Keras {}'.format(tf.keras.__version__))"]},{"cell_type":"markdown","metadata":{"id":"zUvw3mwvfCMA"},"source":["## DRIVE "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39701,"status":"ok","timestamp":1635800324479,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"kkWUwK_IfBtx","outputId":"27c91aa9-7a08-4e6e-b8ac-8deda20cc137"},"outputs":[],"source":["'''MOUNTH THE DRIVE FOLDER TO INTERACT WITH IMAGES AND TO SAVE/LOAD MODELS AND PARAMETERS'''\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","path_drive = '/content/drive/MyDrive/'\n","path = path_drive+'ProgettoDL/'"]},{"cell_type":"markdown","metadata":{"id":"YEzVkjhHbISE"},"source":["## Parametri Immagini "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1635800324481,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"fSyxD4LWbG3J"},"outputs":[],"source":["'''DEFINE VARIABLES AND PARAMETERS TO COLLECT THE INFORMATIONS FROM GOOGLE DRIVE'''\n","\n","'''define a path for the collection of informations (CSV file) for the creation of the dataframe'''\n","os.chdir('/content/drive/MyDrive/ProgettoDL/') \n","\n","'''to have always the same sequence of randomized values (numbers)'''\n","random_state = 6  \n","\n","'''some useful parameters and variables'''\n","parte = 'CALCIO'\n","tipo = 'CROP' #CROP, CROP_gray_ridge\n","augment = True\n","metaclassi = False\n","cnn = \"vgg16\" #resnet50 \n","\n","'''series of production & quality classes of the wood rifle butt'''\n","#classi = ['1','2','3','4']    \n","classi = ['1','2-','2','2+','3-','3','3+','4-','4','4+']          \n","serie = [2,4,8,10,6,9,3,11,12,13,14,15,7] \n","\n","cod_componente = [ 2,  4,  8, 10,  6,  9,  3, 11, 12, 13, 14, 15,  7] #forse non serve\n","\n","'''size of the images & their paths (location) '''\n","immg_rows = 270 \n","immg_cols = 470\n","immgs = '{}_{}'.format(parte,tipo)\n","path_imgs = os.path.join(path_drive+'{}'.format(immgs))\n","\n","'''CSV loading (reading annotations/attributes/informations)'''\n","csv = pd.read_csv(('/content/drive/MyDrive/ProgettoDL/20201102_ExportDB.txt'), sep=\";\")\n","\n","'''check if we're working with coloured or gray images'''\n","if 'gray' in tipo:\n","  colormode = \"grayscale\"\n","  print('analisi in scala di grigi')\n","else:\n","  colormode = \"rgb\""]},{"cell_type":"markdown","metadata":{"id":"xuS19wSXbdE8"},"source":["## SPLIT DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":898,"status":"ok","timestamp":1635800325772,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"t0JSykxbbeCU","outputId":"7f1a821f-9512-4e5e-99a4-03cdb2171d8e"},"outputs":[],"source":["'''CUSTOM SPLIT DATA INTO TRAIN/TEST/VALIDATION SETS'''\n","\"\"\"\n","NOTE\n","- VERSIONE CON NUMERI PRESI DIRETTAMENTE DAL BILANCIAMENTO CALCOLATO RISPETTO IL TOTALE DI 2120 (che ci sono in questo progetto), PER RENDERLO DINAMICO CALCOLARE PESI IN MODO AUTOMATICO (STUDIA ALTERNATIVA)\n","- UNICO PROBLEMA È CHE A VOLTE IMMAGINI CON STESSO ID HANNO IN REALTÀ DIVERSA CLASSE DI QUALITÀ, QUINDI I DATASET NON SONO PERFETTAMENTE BILANCIATI MA VARIANO LEGGERMENTE,\n","(perchè lo stesso ID deve stare in stesso set anche se i lati del calcio del fucile possono avere qualità differente)\n","\"\"\"\n","\n","'''split method'''\n","def split_data(dataframe_result, val_size, test_size, random_state):\n","  classes_count = dataframe_result.groupby(['class']).size() \n","\n","  unique_result, counts = np.unique(dataframe_result['ID'], return_counts=True)   #conto quanti ID univoci esistono nel dataset e li raccolgo tutti in vettore\n","\n","  '''randomizing the order of the IDs, (to change the sequence change the random_state)'''\n","  id_perm = np.random.RandomState(random_state).permutation(unique_result)\n","               \n","  '''define finals sub-sets of data'''\n","  column_names = ['ID','series','filename','class']\n","  x_train = pd.DataFrame(columns = column_names)\n","  x_test = pd.DataFrame(columns = column_names)\n","  x_val = pd.DataFrame(columns = column_names)\n","\n","  '''define variables to count elements inside the sub-sets'''\n","  conta, conta0, conta1, conta2, conta3, conta4, conta5, conta6, conta7, conta8, conta9 = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n","  class_ = 0\n","\n","  '''performing cycles to divide the images into the 3 sub-sets'''\n","  for i in id_perm:\n","     result_ID = dataframe_result.loc[(dataframe_result['ID'] == i)]\n","      #print('Size : {} '.format(result_ID[result_ID.columns[0]].count()))\n","     if result_ID[result_ID.columns[0]].count() == 2:\n","       row_1=result_ID.iloc[0]\n","       class_ = int(row_1['class'])\n","       row_2=result_ID.iloc[1]\n","       class2_ = int(row_2['class'])\n","       conta = 2\n","       #print(\"ID doppio\")\n","     else:\n","       row_1=result_ID.iloc[0]\n","       class_ = int(row_1['class'])\n","       conta = 1\n","       #print(\"ID singolo\")\n","\n","     if class_ == 0 and conta0 < int((classes_count[0]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta0 = conta0 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         conta0 = conta0 + 1\n","     elif class_ == 1 and conta1 < int((classes_count[1]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta1 = conta1 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         conta1 = conta1 + 1\n","     elif class_ == 2 and conta2 < int((classes_count[2]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta2 = conta2 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         conta2 = conta2 + 1\n","     elif class_ == 3 and conta3 < int((classes_count[3]/100)*60) :\n","        if conta == 2 :\n","          x_train=x_train.append(row_1, ignore_index=True) \n","          x_train=x_train.append(row_2, ignore_index=True)\n","          conta3 = conta3 + 2\n","        else :\n","          x_train=x_train.append(row_1, ignore_index=True) \n","          conta3 = conta3 + 1\n","     elif class_ == 4 and conta4 < int((classes_count[4]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta4 = conta4 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True)\n","         conta4 = conta4 + 1 \n","     elif class_ == 5 and conta5 < int((classes_count[5]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta5 = conta5 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         conta5 = conta5 + 1\n","     elif class_ == 6 and conta6 < int((classes_count[6]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta6 = conta6 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         conta6 = conta6 + 1\n","     elif class_ == 7 and conta7 < int((classes_count[7]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta7 = conta7 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         conta7 = conta7 + 1\n","     elif class_ == 8 and conta8 < int((classes_count[8]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta8 = conta8 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         conta8 = conta8 + 1\n","     elif class_ == 9 and conta9 < int((classes_count[9]/100)*60) :\n","       if conta == 2 :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         x_train=x_train.append(row_2, ignore_index=True)\n","         conta9 = conta9 + 2\n","       else :\n","         x_train=x_train.append(row_1, ignore_index=True) \n","         conta9 = conta9 + 1\n","     elif class_ == 0 and conta0 >= int((classes_count[0]/100)*60) and conta0 < int((classes_count[0]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta0 = conta0 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         conta0 = conta0 + 1\n","     elif class_ == 1 and conta1 >= int((classes_count[1]/100)*60) and conta1 < int((classes_count[1]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta1 = conta1 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         conta1 = conta1 + 1\n","     elif class_ == 2 and conta2 >= int((classes_count[2]/100)*60) and conta2 < int((classes_count[2]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta2 = conta2 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         conta2 = conta2 + 1\n","     elif class_ == 3 and conta3 >= int((classes_count[3]/100)*60) and conta3 < int((classes_count[3]/100)*80) :\n","        if conta == 2 :\n","          x_val=x_val.append(row_1, ignore_index=True) \n","          x_val=x_val.append(row_2, ignore_index=True)\n","          conta3 = conta3 + 2\n","        else :\n","          x_val=x_val.append(row_1, ignore_index=True) \n","          conta3 = conta3 + 1\n","     elif class_ == 4 and conta4 >= int((classes_count[4]/100)*60) and conta4 < int((classes_count[4]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta4 = conta4 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True)\n","         conta4 = conta4 + 1 \n","     elif class_ == 5 and conta5 >= int((classes_count[5]/100)*60) and conta5 < int((classes_count[5]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta5 = conta5 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         conta5 = conta5 + 1\n","     elif class_ == 6 and conta6 >= int((classes_count[6]/100)*60) and conta6 < int((classes_count[6]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta6 = conta6 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         conta6 = conta6 + 1\n","     elif class_ == 7 and conta7 >= int((classes_count[7]/100)*60) and conta7 < int((classes_count[7]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta7 = conta7 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         conta7 = conta7 + 1\n","     elif class_ == 8 and conta8 >= int((classes_count[8]/100)*60) and conta8 < int((classes_count[8]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta8 = conta8 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         conta8 = conta8 + 1\n","     elif class_ == 9 and conta9 >= int((classes_count[9]/100)*60) and conta9 < int((classes_count[9]/100)*80) :\n","       if conta == 2 :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         x_val=x_val.append(row_2, ignore_index=True)\n","         conta9 = conta9 + 2\n","       else :\n","         x_val=x_val.append(row_1, ignore_index=True) \n","         conta9 = conta9 + 1\n","     elif class_ == 0 and conta0 >= int((classes_count[0]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta0 = conta0 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         conta0 = conta0 + 1\n","     elif class_ == 1 and conta1 >= int((classes_count[1]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta1 = conta1 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         conta1 = conta1 + 1\n","     elif class_ == 2 and conta2 >= int((classes_count[2]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta2 = conta2 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         conta2 = conta2 + 1\n","     elif class_ == 3 and conta3 >= int((classes_count[3]/100)*80) :\n","        if conta == 2 :\n","          x_test=x_test.append(row_1, ignore_index=True) \n","          x_test=x_test.append(row_2, ignore_index=True)\n","          conta3 = conta3 + 2\n","        else :\n","          x_test=x_test.append(row_1, ignore_index=True) \n","          conta3 = conta3 + 1\n","     elif class_ == 4 and conta4 >= int((classes_count[4]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta4 = conta4 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True)\n","         conta4 = conta4 + 1 \n","     elif class_ == 5 and conta5 >= int((classes_count[5]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta5 = conta5 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         conta5 = conta5 + 1\n","     elif class_ == 6 and conta6 >= int((classes_count[6]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta6 = conta6 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         conta6 = conta6 + 1\n","     elif class_ == 7 and conta7 >= int((classes_count[7]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta7 = conta7 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         conta7 = conta7 + 1\n","     elif class_ == 8 and conta8 >= int((classes_count[8]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta8 = conta8 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         conta8 = conta8 + 1\n","     elif class_ == 9 and conta9 >= int((classes_count[9]/100)*80) :\n","       if conta == 2 :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         x_test=x_test.append(row_2, ignore_index=True)\n","         conta9 = conta9 + 2\n","       else :\n","         x_test=x_test.append(row_1, ignore_index=True) \n","         conta9 = conta9 + 1\n","\n","    \n","  return x_train, x_test, x_val"]},{"cell_type":"markdown","metadata":{"id":"G4OSrOanfe7I"},"source":["## DATA GENERATION "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1635800325773,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"ScS3q5MmWMJg"},"outputs":[],"source":["'''CUSTOM DATA GENERATOR CLASS AND DATA AUGMENTATION METHODS'''\n","import os\n","import pandas as pd\n","import numpy as np\n","import keras\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from sklearn.utils.class_weight import compute_sample_weight\n","from sklearn.utils import class_weight\n","\n","class CustomDataGen(tf.keras.utils.Sequence):\n","    '''constructor method'''\n","    def __init__(self, df, X_col, y_col,\n","                 \n","                 batch_size,\n","                 input_size = (495, 1050), #era 270 x 470\n","                 shuffle = True,\n","                 class_weights = None):\n","      \n","        \n","        self.df = df.copy()\n","        self.X_col = X_col\n","        self.y_col = y_col\n","        self.batch_size = batch_size\n","        self.input_size = input_size\n","        self.shuffle = shuffle\n","        self.class_weights = class_weights\n","        \n","        self.n = len(self.df)\n","        self.n_CLASSE_CALCIO = df[y_col['CLASSE']].nunique()\n","        self.classi_augmented = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}  #to count wich quality classes are augmented during the training on the fly\n","        #self.classi_augmented = {0: 0, 1: 0, 2: 0, 3: 0}  #to count wich quality classes are augmented during the training on the fly\n","\n","        self.n_SERIE_CALCIO = df[y_col['GEOMETRIA']].nunique()\n","        self.serie_augmented = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0}  #to count wich series are augmented during the training on the fly (NOT USED IN THE CODE)\n","\n","        '''augmentation method'''\n","        self.augmentor = ImageDataGenerator(\n","            print(\"[INFO] performing 'on the fly' data augmentation\"),   #verification of effective execution\n","            horizontal_flip = True,   #we augment images by flipping them\n","            brightness_range = [0.2,0.5], #change brightness\n","            preprocessing_function = None,\n","            fill_mode = 'constant',\n","            #rescale=1. / 255,\n","            cval = 0.0\n","        )\n","\n","        '''if weighted when called 'model.fit' the model will perform the training with augmentation, in other case without augmentation'''\n","        if self.class_weights is not None:\n","          self.df2 = df.copy()\n","          self.class_weights = class_weights.copy()\n","    \n","    #def on_epoch_end(self):\n","     #   if self.shuffle:\n","      #      self.df.sample(frac=1).reset_index(drop=True)\n","    \n","    '''load the images and manipulate it'''\n","    def __get_input(self, path, target_size):\n","      try:\n","        if os.path.exists('/content/drive/MyDrive/CALCIO_NOPRE/'+path) != False: \n","          try: \n","            image = tf.keras.preprocessing.image.load_img('/content/drive/MyDrive/CALCIO_NOPRE/'+path, color_mode=\"rgb\" , target_size=(target_size[0],target_size[1]), interpolation=\"nearest\")\n","          except Exception:\n","            print('Errore Image NOT LOAD')\n","      except Exception:\n","        print('\\n{} not found'.format(path))\n","      \n","      image_arr = tf.keras.preprocessing.image.img_to_array(image)\n","      image_arr = tf.keras.applications.vgg16.preprocess_input(image_arr)            \n","      image_arr = tf.image.resize(image_arr,(target_size[0], target_size[1])).numpy()\n","\n","      return image_arr/255.  \n","    \n","    '''quality classes transformed to categorical in order to correctly use the loss=categorical_crossentropy'''\n","    def __get_output(self, label, num_classes):\n","        return tf.keras.utils.to_categorical(label, num_classes=num_classes)\n","\n","    '''series of production transformed to categorical in order to correctly use the loss=categorical_crossentropy'''\n","    def __get_output2(self, label, num_series):\n","        return tf.keras.utils.to_categorical(label, num_classes=num_series)\n","    \n","    '''given a SINGLE batch, the method call other methods to pre-process the images and the informations'''\n","    def __get_data(self, batches):\n","        # Generates data containing batch_size samples\n","\n","        path_batch = batches[self.X_col['PATH_IMG']]  #column where to get the informations\n","        CLASSE_batch = batches[self.y_col['CLASSE']]\n","        SERIE_batch = batches[self.y_col['GEOMETRIA']]\n","        \n","        X_batch = np.asarray([self.__get_input(x, self.input_size) for x in path_batch])\n","        y_batch_ = np.asarray([self.__get_output(y, self.n_CLASSE_CALCIO) for y in CLASSE_batch])\n","        z_batch_ = np.asarray([self.__get_output2(z, self.n_SERIE_CALCIO) for z in SERIE_batch])\n","        y_batch = [y_batch_, z_batch_]  #architecture with multioutput (2 distinct outputs: series, quality_classes), so we need to connect them into a vector \n","        \n","        return X_batch, y_batch\n","    \n","    '''principal method who calls iteratively all the other methods'''\n","    def __getitem__(self, index):      \n","        batches = self.df[index * self.batch_size:(index + 1) * self.batch_size]  #calculate the batch indeces\n","        \n","        X, y = self.__get_data(batches) #getting the effective data pre-processed\n","\n","        '''performing data augmentation'''\n","        if self.class_weights is not None:\n","          sample_weighttt = self.__get_samples_weights_V2(y[0])\n","          X_gen = self.augmentor.flow(X, batch_size=self.batch_size, shuffle=False, sample_weight=sample_weighttt)#, save_to_dir=\"/content/drive/MyDrive/Augmentation\")\n","          return next(X_gen), y      # - next ritorna iterativamente ogni elemento creato con l'augmentation - https://www.w3schools.com/python/ref_func_next.asp\n","        else:              \n","          return X, y   #X è input (immagini), y sono gli output (classi,serie)\n","\n","    '''return length of the batch'''\n","    def __len__(self):\n","        return int(self.n) // self.batch_size\n","\n","    '''calculate sample weights from the elements of each precise batch to perform correctly the augmentation'''\n","    def __get_samples_weights_V2(self, y):\n","      labels = []\n","      for x_row in y: \n","        #print(' y : {}'.format(y[0]))\n","        #print(x_row[0].shape)\n","        class_array = np.where(x_row == 1)  \n","        classe_ = class_array[0]\n","        labels.append(classe_[0])\n","      \n","      labels_batch = np.array(labels)          \n","      \n","      class_weight_present_batch = { your_key: self.class_weights[your_key] for your_key in np.unique(labels_batch)}\n","      \n","      weights = compute_sample_weight(class_weight_present_batch,  labels_batch)\n","\n","      for your_key in np.unique(labels_batch):\n","        self.classi_augmented[your_key] +=1\n","    \n","      weights = np.array(weights)\n","      return weights\n"]},{"cell_type":"markdown","metadata":{"id":"Xzh70UKnU-fM"},"source":["## PREPROCESSING IMAGES & DATA FRAME CREATION"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17557,"status":"ok","timestamp":1635800343757,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"TGKxig25WSRO","outputId":"b17cb9b1-fb61-446f-de0a-30ecf1c0d4cd"},"outputs":[],"source":["'''PREPROCESSING PHASE OF THE DATAFRAME (CREATIONS OF THE SUBSETS TRAIN/VALIDATION/TEST, CALCULATE WEIGHTS OF ELEMENTS OF THE SUBSETS, VERIFY THAT SAME IDs ARE IN THE SAME SUBSET)'''\n","from keras.models import Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import class_weight\n","from pandas.compat._optional import import_optional_dependency\n","\n","os.chdir('/content/drive/MyDrive/ProgettoDL')\n","path = os.getcwd()\n","\n","'''reading inforamtions from the CSV'''\n","col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n","dataframe_sx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n","\n","col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n","dataframe_dx = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n","\n","'''rename the dataframe columns'''\n","dataframe_sx.columns = ['ID','series', 'filename', 'class']\n","dataframe_dx.columns = ['ID','series', 'filename', 'class']\n","\n","frames = [dataframe_sx, dataframe_dx] \n","result = pd.concat(frames) #concatenate the two dataframes\n","\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"DATAFRAME COMPLETO INIZIALE\")\n","print(\"result\")\n","print(result)\n","\n","'''mapping the values used for the classification into integer values'''\n","#version with 10 classes\n","result[\"class\"] = result[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n","result[\"series\"] = result[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)})\n","\n","'''identification of NULL values that would bring the execution on failing and eliminate those values'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"Number of Null values in column 'quality_classes' : \"+format(result['class'].isnull().sum()))\n","print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n","print(\"mostro quegli elementi che hanno valore nullo\")\n","print(result[result['class'].isnull()])\n","print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n","\n","'''Remove Null elements to avoid failures during executions (data in not useful!)'''\n","print(\"Rimuovo gli elementi nulli e verifico stampando nuovamente i valori nulli:\")\n","result['class'] = pd.to_numeric(result['class'], errors='coerce')\n","result = result.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n","\n","print(\"elementi nulli rimasti: \"+format(result['class'].isnull().sum()))     #stampo per verifica se ci sono elementi nulli\n","\n","'''verify if images exist in the Google Drive folder, when not present it is eliminated from the dataset aswell'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"elimino i file che non sono presenti in Google Drive anche se ci sono nel CSV\")\n","import os.path\n","from os import path\n","os.chdir('/content/drive/MyDrive/CALCIO_NOPRE/')\n","#print(os.listdir('/content/drive/MyDrive/CALCIO_NOPRE'))\n","print('CHECK FILE NON PRESENTI NELLA CARTELLA')\n","i = 0; \n","for index, row in result.iterrows():\n","    filename = row['filename']\n","    if os.path.exists('/content/drive/MyDrive/CALCIO_NOPRE/'+filename) == False:\n","      print('File Non Esiste !!!')\n","    if(os.path.exists(filename) == False):\n","      result = result.drop(result[(result['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","      i = i + 1             \n","print('File Eliminati : {} '.format(i))\n","\n","print('CHECK FILE CON NaN')\n","print(result[result['class'].isnull()])\n","print(result[result['series'].isnull()])\n","print(result[result['filename'].isnull()])\n","print(result[result['ID'].isnull()])\n","result = result[result['class'].notna()]\n","result = result[result['series'].notna()]\n","result = result[result['filename'].notna()]\n","result = result[result['ID'].notna()]\n","\n","\n","\n","\n","'''creation of masked images (grayscale images) and save them in Google Drive'''\n","'''than create a second dataframe with these new images'''\n","#print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","#mask_filenames = []\n","#IDs = []\n","#classes = []\n","#for index, row in result.iterrows():\n","#    filename = row['filename']\n","#    mask_filenames.append(str(\"mask_\"+filename))\n","#    IDs.append(row['ID'])\n","#    classes.append(row['class'])\n","\n","#print(\"DATAFRAME CON MASCHERE\")\n","#result2 = result.copy()\n","#result2['mask_filename'] = mask_filenames\n","#result2.drop('filename', axis='columns', inplace=True)   #rimuovo colonna con path immagini normali\n","\n","#column_names = [\"ID\",\"series\", \"mask_filename\", \"class\"]\n","#result2 = result2.reindex(columns=column_names)\n","\n","#print(\"result2\")\n","#print(result2)\n","\n","'''performing the splitting of the dataframe into sub-sets'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","print(\"SPLIT DATA\")\n","train_balance_df, test_balance_df, val_balance_df  = split_data(result, 0.2, 0.2, 3)  #CUSTOM SPLIT CON ID IN STESSO SET DI DATI\n","#train_mask, test_mask, validation_mask  = split_data(result2, 0.2, 0.2, 3)           #split per test con immagini con maschere\n","\n","print(\"train_balance_df\")\n","print(train_balance_df.shape)\n","print(\"test_balance_df\")\n","print(test_balance_df.shape)\n","print(\"val_balance_df\")\n","print(val_balance_df.shape)\n","\n","\n","#------------------------version with 4 classes (togliere se si lavora con 10 classi)-----------------------------------\n","#train_balance_df[\"class\"] = train_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","#val_balance_df[\"class\"] = val_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","#test_balance_df[\"class\"] = test_balance_df[\"class\"].map({0: int(0), 1: int(1), 2: int(1), 3: int(1), 4: int(2), 5: int(2), 6: int(2), 7: int(3), 8: int(3), 9: int(3)})\n","\n","\"\"\"\n","NOTA: versione dei metodi di tensorflow, che non divide però mantenendo stessi ID in stessi Sub-set\n","train_balance_df, test_balance_df = train_test_split(result, test_size=0.4, stratify=result['class'], random_state=2)\n","test_balance_df, val_balance_df = train_test_split(test_balance_df, test_size=0.5, stratify=test_balance_df['class'],random_state=2)\n","\"\"\"\n","\n","'''verify distibution of classes in the sub-sets and calculate weights of the classes in each sub-set'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","vals, counts = np.unique(train_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Train\")\n","print(len(train_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts[i]))\n","\n","vals2, counts2 = np.unique(val_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Validation\")\n","print(len(val_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts2[i]))\n","\n","vals3, counts3 = np.unique(test_balance_df['class'], return_counts=True)\n","print(\"conta del numero di immagini per speicfica classe in set Test\")\n","print(len(test_balance_df))\n","for i in range(0,len(classi)):\n","    print('{}:{}'.format(classi[i], counts3[i]))    \n","\n","\n","class_weights_train = class_weight.compute_class_weight('balanced',np.unique(train_balance_df['class']),train_balance_df['class'])\n","weight_train = {i : round(class_weights_train[i], 2) for i in range(len(classi))} \n","print('Weight train_balance_df')\n","print(weight_train)\n","\n","\n","class_weights = class_weight.compute_class_weight('balanced',np.unique(val_balance_df['class']),val_balance_df['class'])\n","weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n","print('Weight val_balance_df')\n","print(weight)\n","\n","\n","class_weights = class_weight.compute_class_weight('balanced',np.unique(test_balance_df['class']),test_balance_df['class'])\n","weight = {i : round(class_weights[i], 2) for i in range(len(classi))} \n","print('Weight test_balance_df')\n","print(weight)\n","\n","\n","'''verify that same IDs are in the same sub-sets'''\n","print(\"------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n","\n","'''method for univoque sets'''\n","def check_for_leakage(df1, df2, patient_col):\n","    \"\"\"\n","    Return True if there any patients are in both df1 and df2.\n","    Args:\n","        df1 (dataframe): dataframe describing first dataset\n","        df2 (dataframe): dataframe describing second dataset\n","        patient_col (str): string name of column with patient IDs\n","    Returns: leakage (bool): True if there is leakage, otherwise False\n","    \"\"\"\n","    df1_patients_unique = set(df1[patient_col])\n","    df2_patients_unique = set(df2[patient_col])\n","    \n","    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)\n","    # leakage contains true if there is patient overlap, otherwise false.\n","    leakage = len(patients_in_both_groups) >= 1 # boolean (true if there is at least 1 patient in both groups)\n","    return leakage\n","\n","\n","#--------verifico che stessi ID siano in stesso set--------\n","print(\"test case 1 - train VS validation\")\n","print(f\"Stessi ID in set usati?: {check_for_leakage(train_balance_df, val_balance_df, 'ID')}\")\n","print(\"-------------------------------------\")\n","print(\"test case 2 - train VS test\")\n","print(f\"Stessi ID in set usati ?: {check_for_leakage(train_balance_df, test_balance_df, 'ID')}\")\n","print(\"-------------------------------------\")\n","print(\"test case 3 - validation VS test\")\n","print(f\"Stessi ID in set usati?: {check_for_leakage(val_balance_df, test_balance_df, 'ID')}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1635800343758,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"DSvo5qiEEeVB","outputId":"3b045635-381c-4e24-f3f5-068cde1f98c4"},"outputs":[],"source":["'''Verifica Classi Qualità per ogni Serie'''\n","print(\"Verifica Classi Qualità per ogni Serie\")\n","result_x_ = result.groupby(['series','class']).size()\n","result_class = result.groupby(['class']).size()\n","print(result_class)\n","result_series = result.groupby(['series']).size()\n","print(result_series)\n","\n","print('SOMMA IMG : {}'.format(result_class[0]+result_class[1]+result_class[2]+result_class[3]+result_class[4]+result_class[5]+result_class[6]+result_class[7]+result_class[8]+result_class[9]))\n","      \n"]},{"cell_type":"markdown","metadata":{"id":"ss65GPosBd3o"},"source":["##Weighted Categorical Cross-Entrophy"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1635800343760,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"zWDxSeamBcw9"},"outputs":[],"source":["from keras import backend as K\n","class weighted_categorical_crossentropy(object):\n","    \"\"\"\n","    A weighted version of keras.objectives.categorical_crossentropy\n","    \n","    Variables:\n","        weights: numpy array of shape (C,) where C is the number of classes\n","    \n","    Usage:\n","        loss = weighted_categorical_crossentropy(weights).loss\n","        model.compile(loss=loss,optimizer='adam')\n","    \"\"\"\n","    \n","    def __init__(self,weights):\n","        self.weights = K.variable(weights)\n","        \n","    def loss(self,y_true, y_pred):\n","        #print('stop')\n","        \n","        # scale preds so that the class probas of each sample sum to 1\n","        y_pred /= K.sum(y_pred)\n","        # clip\n","        y_pred = K.clip(y_pred, K.epsilon(), 1)\n","        # calc\n","        \n","        loss = y_true*K.log(y_pred)*self.weights\n","        loss =-K.sum(loss,-1)\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"28i0TbSpzt_Z"},"source":["## NETWORK "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5629,"status":"ok","timestamp":1635800349381,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"q0Dc7ahEzpWW","outputId":"17fc3383-9c6b-40d4-d74b-6e2f35a47ace"},"outputs":[],"source":["'''NETWORK DEFINITION'''\n","\n","\n","'''build the top model'''\n","model = Sequential()\n","\n","vgg16_conv = VGG16(include_top=False, weights='imagenet', input_shape=(immg_rows, immg_cols, 3))  \n","for layer in vgg16_conv.layers[:-1]:\n","    layer.trainable = False\n","\n","##NOTA : in alcune alternative ho visto che applicano il Flatten prima della ramificazione.\n","top_model = Flatten(name='flatten')(vgg16_conv.output)\n","\n","'''building the first ramification - quality class classificator'''   \n","#x = Flatten(name='flatten')(vgg16_conv.output)\n","x = Dropout(0.5)(top_model)\n","x = Dense(4096, activation='relu', name='fc1')(x)\n","x = Dense(4096, activation='relu', name='fc2')(x)\n","x = BatchNormalization()(x)\n","x = Dense(len(classi), activation='softmax', name='class_output')(x) #-Prediction_QUALITY_CLASS\n","\n","'''building the second ramification - domain adaptator (adversarial debiasing), series class classificator'''\n","#-----------------------\n","#Gradient reversal layer (per invertire il segno della loss e mantenere una forma di Loss complessiva formata solo da somme)\n","\n","@tf.custom_gradient\n","def grad_reverse(x):\n","    y = tf.identity(x)\n","    def custom_grad(dy):\n","        return -dy\n","    return y, custom_grad\n","\n","class GradReverse(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def call(self, x):\n","        return grad_reverse(x)\n","\n","#-----------------------\n","#x2 = Flatten(name='flatten2')(vgg16_conv.output)\n","\n","x2 = GradReverse()(top_model)\n","\n","x2 = Dropout(0.5)(x2)\n","x2 = Dense(4096, activation='relu', name='fc1_2')(x2)\n","x2 = Dense(4096, activation='relu', name='fc2_2')(x2)\n","x2 = BatchNormalization()(x2)\n","x2 =  Dense(len(serie), activation='softmax', name='series_output')(x2)   #-Adversarial_Debiasing\n","\n","\n","'''creating the complete architecture'''\n","\n","dot_img_file = '/tmp/model_2.png'     #choose a location where save the image model\n"," \n","model = keras.Model(vgg16_conv.input, [x, x2], name=\"quality_recognizer\")   #create the complete model ì, with the input and the 2 outputs ramifications\n","\n","model.summary()    # inspect model in output video\n","\n","tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)  #plot the model with the shapes\n"]},{"cell_type":"markdown","metadata":{"id":"BdnARydQAI-f"},"source":["##Metrica Balance Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":263,"status":"ok","timestamp":1635802362855,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"hvHBOEvqAL1Z"},"outputs":[],"source":["import keras.backend as K\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.metrics import confusion_matrix\n","'''\n","Funzione per Balance Accuracy \n","-- Link Utile : https://medium.com/@mostafa.m.ayoub/customize-your-keras-metrics-44ac2e2980bd --\n","-- https://medium.com/analytics-vidhya/custom-metrics-for-keras-tensorflow-ae7036654e05 --- \n","-- https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score --- \n","'''\n","\n","'''\n","def monitor_balance_accuracy ():\n","\tdef bal_acc(y_true, y_pred):\n","    print(\"y_true \"+format.{y_true})\n","    print(\"y_pred \"+format.{y_pred})\n","    C = confusion_matrix(y_true, y_pred)\n","    print(\"CM \"+format.{C})\n","  #with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n","    per_class = tf.linalg.diag_part(C) / K.sum(axis=1)\n","    print(\"per_class \"+format.{per_class})\n","\t\ttp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","\t\tfn = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)\n","\t\t#sensitivity = tp / (fn + K.epsilon()) #--- primo test : versione trovata sul web, ma non tornano le formule\n","\t\tsensitivity = tp / (fn + tp + K.epsilon()) #--OK\n","\n","\t\ttn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n","\t\tfp = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n","\t\t#specificity = tn / (fp + K.epsilon()) #--- primo test : versione trovata sul web, ma non tornano le formule \n","\t\tspecificity = tn / (fp + tn + K.epsilon()) #--OK \n","\t\n","\t\tBalanced_Accuracy = (sensitivity+specificity)/2 #--OK\n","\t\treturn Balanced_Accuracy \n","\treturn bal_acc\n","'''\n","\n","def monitor_balance_accuracy ():\n","  def class_output_bal_acc(y_true, y_pred):\n","    Recall0=0\n","    Recall1=0\n","    Recall2=0\n","    Recall3=0\n","    Recall4=0\n","    Recall5=0\n","    Recall6=0\n","    Recall7=0\n","    Recall8=0\n","    Recall9=0\n","\n","    Recall0 = tf.keras.metrics.Recall(class_id=0)\n","    Recall0.update_state(y_true, y_pred)\n","    Recall1 = tf.keras.metrics.Recall(class_id=1)\n","    Recall1.update_state(y_true, y_pred)\n","    #rec1 = Recall1.result()\n","    Recall2 = tf.keras.metrics.Recall(class_id=2)\n","    Recall2.update_state(y_true, y_pred)\n","    Recall3 = tf.keras.metrics.Recall(class_id=3)\n","    Recall3.update_state(y_true, y_pred)\n","    Recall4 = tf.keras.metrics.Recall(class_id=4)\n","    Recall4.update_state(y_true, y_pred)\n","    Recall5 = tf.keras.metrics.Recall(class_id=5)\n","    Recall5.update_state(y_true, y_pred)\n","    Recall6 = tf.keras.metrics.Recall(class_id=6)\n","    Recall6.update_state(y_true, y_pred)\n","    Recall7 = tf.keras.metrics.Recall(class_id=7)\n","    Recall7.update_state(y_true, y_pred)\n","    Recall8 = tf.keras.metrics.Recall(class_id=8)\n","    Recall8.update_state(y_true, y_pred)\n","    Recall9 = tf.keras.metrics.Recall(class_id=9)\n","    Recall9.update_state(y_true, y_pred)\n","\n","    #Mean = K.sum(Recall1.result(),Recall2.result(),Recall3.result(),Recall4.result(),Recall5.result(),Recall6.result(),Recall7.result(),Recall8.result(),Recall9.result())\n","    Bal_acc = tf.reduce_mean([K.cast(Recall0.result(), dtype='float32'),K.cast(Recall1.result(), dtype='float32'),K.cast(Recall2.result(), dtype='float32'),\n","                              K.cast(Recall3.result(), dtype='float32'),K.cast(Recall4.result(), dtype='float32'),K.cast(Recall5.result(), dtype='float32'),\n","                              K.cast(Recall6.result(), dtype='float32'),K.cast(Recall7.result(), dtype='float32'),K.cast(Recall8.result(), dtype='float32'),K.cast(Recall9.result(), dtype='float32'),], keepdims=False)\n","    return (Bal_acc) \n","  return class_output_bal_acc\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VX2SB5uYzzyB"},"source":["## Callbacks "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":360,"status":"ok","timestamp":1635802368768,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"gvAnOHC2W134"},"outputs":[],"source":["'''CALLBACKS'''\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","#NB: create sul drive una cartella weights dove salvare i pesi durante l'allenamento\n","path_drive = '/content/drive/My Drive/'\n","path = path_drive+'ProgettoDL/'\n","\n","#model_checkpoint = ModelCheckpoint( filepath=os.path.join('/content/drive/My Drive/ProgettoDL/weights/model_{}_{}/best_weights.h5'.format(immgs,cnn)), monitor='val_loss', verbose=1, save_best_only=True)\n","model_checkpoint_val_bal_acc = ModelCheckpoint( filepath=os.path.join('/content/drive/My Drive/ProgettoDL/weights/model_{}_{}/best_weights.h5'.format(immgs,cnn)), monitor='val_bal_acc', verbose=1, save_best_only=True)\n","model_checkpoint_val_loss = ModelCheckpoint( filepath=os.path.join('/content/drive/My Drive/ProgettoDL/weights/model_{}_{}/best_weights.h5'.format(immgs,cnn)), monitor='val_loss', verbose=1, save_best_only=True)\n","\n","#early_stopping = EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n","\n","### MODIFICATO QUA - Implementazione Early Stopping###\n","early_stopping_val_bal_acc = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_bal_acc', #Quantity to be monitored \n","    min_delta=0, #Minimum change in the monitored quantity to qualify as an improvement\n","    patience=30, #Number of epochs with no improvement after which training will be stopped\n","    #verbosity mode, setting verbose 0, 1 or 2 you just say \n","    #how do you want to 'see' the training progress for each epoch.\n","    #verbose=0 will show you nothing (silent)\n","    #verbose=1 will show you an animated progress bar like this: progres_bar\n","    verbose=0, \n","    #Mode = One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity \n","    #monitored has stopped decreasing; in \"max\" mode \n","    #it will stop when the quantity monitored has stopped increasing; \n","    #in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n","    mode=\"auto\",\n","    #Training will stop if the model doesn't show improvement over the baseline.\n","    baseline=None,\n","    #Whether to restore model weights from the epoch with the best value of the monitored quantity\n","    restore_best_weights=False\n",")\n","early_stopping_val_loss = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', #Quantity to be monitored \n","    min_delta=0, #Minimum change in the monitored quantity to qualify as an improvement\n","    patience=30, #Number of epochs with no improvement after which training will be stopped\n","    #verbosity mode, setting verbose 0, 1 or 2 you just say \n","    #how do you want to 'see' the training progress for each epoch.\n","    #verbose=0 will show you nothing (silent)\n","    #verbose=1 will show you an animated progress bar like this: progres_bar\n","    verbose=0, \n","    #Mode = One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity \n","    #monitored has stopped decreasing; in \"max\" mode \n","    #it will stop when the quantity monitored has stopped increasing; \n","    #in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n","    mode=\"auto\",\n","    #Training will stop if the model doesn't show improvement over the baseline.\n","    baseline=None,\n","    #Whether to restore model weights from the epoch with the best value of the monitored quantity\n","    restore_best_weights=False\n",")\n","callbacks=[model_checkpoint_val_bal_acc, model_checkpoint_val_loss , early_stopping_val_loss, early_stopping_val_bal_acc ]"]},{"cell_type":"markdown","metadata":{"id":"Ks3Fly2oUf3g"},"source":["##HYPERPARAMETERS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1635802372635,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"Bbjwj4QwW6yx","outputId":"74db972f-1a06-4ed3-a2c3-e2ba0b66257a"},"outputs":[],"source":["'''HYPERPARAMETERS DEFINITION'''\n","#per la stampa dei tensori \n","import keras.backend as K   \n","test_array_s0 = []\n","pred_array_s0 = []\n","i=0\n","\n","#opt = Adam(learning_rate=0.00001)\n","opt = SGD(learning_rate = 0.001, decay = 1e-5, momentum= 0.8)\n","\n","num_epochs = 2 #era 100\n","#num_epochs = 100 #era 100\n","bs = 16 #era 16 \n","#model.compile(loss=loss, optimizer=opt, metrics = ['accuracy']) \n","\n","# To do : capire come dividere y_pred, y_true per ogni serie \n","# https://github.com/aws-samples/amazon-sagemaker-custom-loss-function/blob/master/custom-loss/Training_Models_with_Unequal_Economic_Error_Costs.ipynb\n","\n","\n","#losses = {\n","#\t\"class_output\": \"categorical_crossentropy\",\n","# \"series_output\": \"categorical_crossentropy\",\n","#}\n","class_weights = weight_train\n","print('Class Weight Train : {} '.format(class_weights))\n","print('Type Class Weight Train : {} '.format(type(class_weights)))\n","\n","data = list(class_weights.values())\n","class_weights = np. array(data)\n","print('Class Weight Train : {} '.format(class_weights))\n","\n","losses = {\n","\t\"class_output\": weighted_categorical_crossentropy(class_weights).loss,\n","  \"series_output\": \"categorical_crossentropy\" \n","}\n","\n","lossWeights = {\"class_output\": 1.0, \"series_output\": 0.5}\n","\n","model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights, metrics=[\"accuracy\",monitor_balance_accuracy()], run_eagerly=True)\n"," "]},{"cell_type":"markdown","metadata":{"id":"bfJ3LnbAZhQx"},"source":["## Testing Model "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":454462,"status":"ok","timestamp":1635802833129,"user":{"displayName":"Denis Bernovschi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghw-U9nWlIo1w_VU6NQkSdNyp0y0fF_0QpNd2cq=s64","userId":"01730545031398802449"},"user_tz":-60},"id":"UMXAiMAvyom5","outputId":"6fad71bd-36c5-47fb-c658-4290c7154644"},"outputs":[],"source":["'''TRAINING THE MODEL'''\n","from sklearn.preprocessing import LabelBinarizer\n","\n","\"\"\"\n","#PARTE ALLENAMENTO CON IMMAGINI MASCHERATE\n","traingen = CustomDataGen(train_balance_df, X_col={'PATH_IMG':'mask_filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470), class_weights = weight_train)       #verifica weights train\n","testgen = CustomDataGen(test_balance_df, X_col={'PATH_IMG':'mask_filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470))   \n","valgen = CustomDataGen(val_balance_df, X_col={'PATH_IMG':'mask_filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470))\n","\"\"\"\n","\n","#PARTE ALLENAMENTO IMMAGINI NORMALI\n","traingen = CustomDataGen(train_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470), class_weights = weight_train)       #verifica weights train\n","testgen = CustomDataGen(test_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470))   \n","valgen = CustomDataGen(val_balance_df, X_col={'PATH_IMG':'filename'}, y_col={'CLASSE': 'class', 'GEOMETRIA': 'series'}, batch_size=bs, input_size=(270,470))\n","\n","\n","\n","#history = model.fit(traingen,validation_data=valgen, epochs=num_epochs, callbacks = callbacks, verbose=1)       #OLD VERSION\n","\n","history = model.fit(x=traingen,validation_data=valgen, epochs=num_epochs, callbacks = callbacks, verbose=1)\n","print(history.history.keys()) #---serve per stampare le metriche che ho nel modello \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0uCfdpQ9Sh_"},"outputs":[],"source":["print(\"Conta del numero di immagini per specifica classe in set Train\")\n","print(traingen.classi_augmented)\n","\n","#####QUESTI PRINT SERVONO SOLO DI VERIFICA SE SOPRA STAMPA CORRETTAMENTE ... GLI ELEMENTI DI SERIE 1 (SONO DEI TEST) UNA VOLTA FINITA CANCELLARE\n","k = 0\n","for index, row in test_balance_df.iterrows():\n","            series_ = int(row['series'])\n","            if series_ == 0:\n","              k=k+1\n","print('k - test : {}'.format(k))\n","\n","k = 0\n","for index, row in train_balance_df.iterrows():\n","            series_ = int(row['series'])\n","            if series_ == 0:\n","              k=k+1\n","print('k - test : {}'.format(k))"]},{"cell_type":"markdown","metadata":{"id":"xBGTOuc1VSqq"},"source":["## PLOT "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQzh_8drJ56m"},"outputs":[],"source":["'''PLOT CURVES'''\n","\n","path = path_drive+'ProgettoDL/'\n","\n","acc_class = history.history['class_output_accuracy']\n","acc_series = history.history['series_output_accuracy']\n","#acc = history.history['accuracy'] ##---- modificato sopra\n","#val_acc = history.history['val_accuracy'] #---- modificato sotto \n","val_acc_class = history.history['val_class_output_accuracy']\n","val_acc_series = history.history['val_series_output_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","class_output_bal_acc = history.history['class_output_bal_acc']\n","val_class_output_bal_acc = history.history['val_class_output_bal_acc']\n","lista = [acc_class,acc_series,val_acc_class,val_acc_series,loss,val_loss,class_output_bal_acc,val_class_output_bal_acc] #--- modificato \n","\n","\n","#print(corr(history.history['accuracy'], history_mask.history_mask['accuracy']))\n","\n","import csv\n","\n","with open(\"VGG16.csv\", \"w\", newline=\"\") as f:\n","    writer = csv.writer(f)\n","    writer.writerows(lista)\n","     \n","epochs = range(len(acc_class))\n","\n","plt.plot(epochs, acc_class, 'r', label='Training acc Class') \n","plt.plot(epochs, acc_series, 'g', label='Training acc Series') \n","plt.plot(epochs, val_acc_class, 'b', label='Validation acc Class') \n","plt.plot(epochs, val_acc_series, 'y', label='Validation acc Series')\n","plt.title('Training and validation accuracy (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path+'weights/PlotAcc_{}_{}.pdf'.format(immgs,cnn))) \n","\n","plt.figure()\n"," \n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path+'weights/PlotLoss_{}_{}.pdf'.format(immgs,cnn)))\n","\n","plt.figure()\n","\n","plt.plot(epochs, class_output_bal_acc, 'b', label='Training Balance Accuracy')\n","plt.plot(epochs, val_class_output_bal_acc, 'r', label='Validation Balance Accuracy')\n","plt.title('Training and validation balance accuracy (IMG)')\n","plt.legend()\n","plt.savefig(os.path.join(path+'weights/PlotLoss_{}_{}.pdf'.format(immgs,cnn)))"]},{"cell_type":"markdown","metadata":{"id":"QjI6Y_nefFTj"},"source":["### SAVE MODEL "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApFaEaa6oPu0"},"outputs":[],"source":["#salvataggio modello pesi finali\n","from tensorflow.keras.models import Sequential, save_model, load_model\n","path = path_drive+'ProgettoDL/'\n","model.save(os.path.join(path+'weights/model_{}_{}/Final'.format(immgs,cnn)))\n","print(\"Saved model to disk\")"]},{"cell_type":"markdown","metadata":{"id":"XefnmV_tsbOw"},"source":["## LOAD MODEL "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VDiBcm-XEOb"},"outputs":[],"source":["'''LOAD THE SAVED MODEL'''\n","import os\n","from tensorflow.keras.models import Sequential, save_model, load_model\n","path_drive = '/content/drive/My Drive/'\n","path = path_drive+'ProgettoDL/'\n","\n","parte = 'CALCIO'\n","tipo = 'CROP' #CROP, CROP_gray_ridge\n","augment = True\n","metaclassi = False\n","cnn = \"vgg16\" #resnet50 \n","\n","#classi = ['1','2','3','3+','4']\n","classi = ['1','2-','2','2+','3-','3','3+','4-','4','4+']\n","\n","immg_rows = 270 \n","immg_cols = 470\n","immgs = '{}_{}'.format(parte,tipo)\n","\n","path_model = os.path.join(path+'weights/model_{}_{}/Final'.format(immgs,cnn))\n","\n","model = load_model(path_model, compile=False) #-- errore con custom loss\n","#model = load_model(path_model, custom_objects={\"CustomModel\": Model} #--- per caricare il modelo in teoria \n","#ValueError: Unknown loss function: custom_loss. Please ensure this object is passed to the `custom_objects` argument. \n","#https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object\n","print('Model IMG Loaded')\n"]},{"cell_type":"markdown","metadata":{"id":"HQMtb_JBD5Vp"},"source":["## PREDICTION "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDb9juVJfFBc"},"outputs":[],"source":["'''EXECUTE PREDICTIONS FROM THE TRAINED MODEL'''\n","\n","from tensorflow.keras.models import Sequential, save_model, load_model\n","from tensorflow.keras import Model\n","from keras.applications.vgg16 import preprocess_input\n","\n","test_array = []\n","test_array_series = []\n","\n","for index, row in test_balance_df.iterrows():\n","    class_ = int(row['class'])\n","    series_ = int(row['series'])          #---da qui e nei prossimi, calcolo la ground thruth del shotgun series, ovvero i semplici COD_COMPONENTE (serie) che appartengono al sub-set di test\n","    test_array.append(class_)\n","    test_array_series.append(series_)     \n","\n","test_array = np.array(test_array)\n","test_array_series = np.array(test_array_series)   \n","\n","y_test = to_categorical(np.unique(test_array, return_inverse=True)[1])\n","y_test_series = to_categorical(np.unique(test_array_series, return_inverse=True)[1]) \n","\n","imgs_array = [] \n","\n","\n","'''carico le immagini del sub-set di test'''\n","for index, row in testgen.df.iterrows():\n","    filename = row['filename']       \n","    image = load_img('/content/drive/MyDrive/CALCIO_NOPRE/{}'.format(filename), target_size = (immg_rows, immg_cols))\n","    x = img_to_array(image)\n","    x = preprocess_input(x)  #non dovrebbe servire\n","    imgs_array.append(x)\n","    X_test = np.asarray(imgs_array)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FpZ8K4yZXZ7"},"outputs":[],"source":["######PARTE DA FINIRE DI SISTEMARE\n","y_test_no_argmax = y_test\n","\n","y_test = y_test.argmax(axis=1)\n","y_test_series = y_test_series.argmax(axis=1)      \n","\n","'''predictions'''\n","y_pred = model.predict(X_test)\n","\n","#---------------\n","#metriche nuove senza usare argmax.\n","#---------------\n","y_pred_no_argmax = y_pred[0]            #--- modificato qua aggiungendo y_pred[0] invece di y_pred\n","y_pred = np.argmax(y_pred[0],axis=1)    #--- modificato qua aggiungendo y_pred[0] invece di y_pred\n","\n","print(y_pred)\n","print(y_test)\n","\n","print(y_pred_no_argmax.shape)\n","print(y_test_no_argmax.shape)\n","\n","#y_pred_conf = model.predict(X_test)\n","#index = np.where(np.equal(y_pred, y_test) == False)[0]\n","#print(np.around(y_pred_conf[index], decimals = 2))\n","\n","\"\"\"\n","#alcune verifiche visive sull'intero sub-set di test\n","print(y_test)\n","print(y_test.shape)\n","print(y_test_series)\n","print(y_test_series.shape)\n","print(y_pred)\n","print(y_pred.shape)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHMuVIu0_ZFJ"},"outputs":[],"source":["dataset_bias_test = test_balance_df.copy()  #10/09\n","dataset_bias_test.scores = y_pred           #10/09\n","#dataset_bias_test.labels = test_balance_df.labels #10/09\n","print(dataset_bias_test.scores)\n","print(dataset_bias_test)"]},{"cell_type":"markdown","metadata":{"id":"VkaCQp4Ndzbe"},"source":["##SEARCH UNIVOQUE SERIES TO BALANCE SETS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkviN4oh2M6l"},"outputs":[],"source":["'''SEARCHING UNIVOQUE SERIES'''\n","\n","'''serve per fare le stampe delle confusion matrix per ciascuna serie di appartenenza delle immagini del sub-set test'''\n","test_array_s0, test_array_s1, test_array_s2, test_array_s3, test_array_s4, test_array_s5, test_array_s6, test_array_s7, test_array_s8, test_array_s9, test_array_s10, test_array_s11, test_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n","pred_array_s0, pred_array_s1, pred_array_s2, pred_array_s3, pred_array_s4, pred_array_s5, pred_array_s6, pred_array_s7, pred_array_s8, pred_array_s9, pred_array_s10, pred_array_s11, pred_array_s12 = [], [], [], [], [], [], [], [], [], [], [], [], []\n","i=0\n","for index, row in test_balance_df.iterrows():\n","    \n","    series_ = int(row['series'])\n","    if series_ == 0:\n","      test_array_s0.append(y_test[i])\n","      pred_array_s0.append(y_pred[i])\n","    if series_ == 1:\n","      test_array_s1.append(y_test[i])\n","      pred_array_s1.append(y_pred[i])\n","    if series_ == 2:\n","      test_array_s2.append(y_test[i])\n","      pred_array_s2.append(y_pred[i])\n","    if series_ == 3:\n","      test_array_s3.append(y_test[i])\n","      pred_array_s3.append(y_pred[i])\n","    if series_ == 4:\n","      test_array_s4.append(y_test[i])\n","      pred_array_s4.append(y_pred[i])\n","    if series_ == 5:\n","      test_array_s5.append(y_test[i])\n","      pred_array_s5.append(y_pred[i])\n","    if series_ == 6:\n","      test_array_s6.append(y_test[i])\n","      pred_array_s6.append(y_pred[i])\n","    if series_ == 7:\n","      test_array_s7.append(y_test[i])\n","      pred_array_s7.append(y_pred[i])\n","    if series_ == 8:\n","      test_array_s8.append(y_test[i])\n","      pred_array_s8.append(y_pred[i])\n","    if series_ == 9:\n","      test_array_s9.append(y_test[i])\n","      pred_array_s9.append(y_pred[i])\n","    if series_ == 10:\n","      test_array_s10.append(y_test[i])\n","      pred_array_s10.append(y_pred[i])\n","    if series_ == 11:\n","      test_array_s11.append(y_test[i])\n","      pred_array_s11.append(y_pred[i])\n","    if series_ == 12:\n","      test_array_s12.append(y_test[i])\n","      pred_array_s12.append(y_pred[i])\n","\n","    i=i+1\n","\n","print(test_array_s0)\n","print(pred_array_s0)\n","\n","from functools import reduce\n","reduced = reduce(np.union1d, (pred_array_s0, test_array_s0))\n","print(reduced)"]},{"cell_type":"markdown","metadata":{"id":"D8WKlbpqKgAP"},"source":["## METRICHE MASK & IMG "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eHSjdjJcgc1Y"},"outputs":[],"source":["'''METRICHE'''\n","print('--------------Metrice IMG----------------')\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score,classification_report, f1_score, precision_score, recall_score, confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","\n","a = accuracy_score(y_test, y_pred)                                              # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n","print(\"test accuracy:\",a)\n","print(\"precision:\", precision_score(y_test, y_pred , average=\"macro\"))          # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html?highlight=precision_score#sklearn.metrics.precision_score\n","print(\"recall:\", recall_score(y_test, y_pred , average=\"macro\"))                # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall_score#sklearn.metrics.recall_score\n","print(\"f1_score:\", f1_score(y_test, y_pred , average=\"macro\"))                  # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score\n","\n","print('classification report:')\n","print(classification_report(y_test, y_pred))  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"tJjt39sGAfQF"},"source":["Per quanto riguarda la funzione np_quadratic_weighted_kappa abbiamo avuto alcune difficoltà implementative e quindi abbiamo cercato un codice online che ci calcolasse la stessa metrica \n","\n","[Link Utilizzato](https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in2Vpij3AHgB"},"outputs":[],"source":["\n","# The following 3 functions have been taken from Ben Hamner's github repository\n","# https://github.com/benhamner/Metrics\n","def Cmatrix(rater_a, rater_b, min_rating=None, max_rating=None):\n","    \"\"\"\n","    Returns the confusion matrix between rater's ratings\n","    \"\"\"\n","    assert(len(rater_a) == len(rater_b))\n","    if min_rating is None:\n","        min_rating = min(rater_a + rater_b)\n","    if max_rating is None:\n","        max_rating = max(rater_a + rater_b)\n","    num_ratings = int(max_rating - min_rating + 1)\n","    conf_mat = [[0 for i in range(num_ratings)]\n","                for j in range(num_ratings)]\n","    for a, b in zip(rater_a, rater_b):\n","        conf_mat[a - min_rating][b - min_rating] += 1\n","    return conf_mat\n","\n","\n","def histogram(ratings, min_rating=None, max_rating=None):\n","    \"\"\"\n","    Returns the counts of each type of rating that a rater made\n","    \"\"\"\n","    if min_rating is None:\n","        min_rating = min(ratings)\n","    if max_rating is None:\n","        max_rating = max(ratings)\n","    num_ratings = int(max_rating - min_rating + 1)\n","    hist_ratings = [0 for x in range(num_ratings)]\n","    for r in ratings:\n","        hist_ratings[r - min_rating] += 1\n","    return hist_ratings\n","\n","def quadratic_weighted_kappa(y, y_pred):\n","    \"\"\"\n","    Calculates the quadratic weighted kappa\n","    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n","    value, which is a measure of inter-rater agreement between two raters\n","    that provide discrete numeric ratings.  Potential values range from -1\n","    (representing complete disagreement) to 1 (representing complete\n","    agreement).  A kappa value of 0 is expected if all agreement is due to\n","    chance.\n","    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n","    each correspond to a list of integer ratings.  These lists must have the\n","    same length.\n","    The ratings should be integers, and it is assumed that they contain\n","    the complete range of possible ratings.\n","    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n","    is the minimum possible rating, and max_rating is the maximum possible\n","    rating\n","    \"\"\"\n","    rater_a = y\n","    rater_b = y_pred\n","    min_rating=1 # era None abbiamo messo 0\n","    max_rating=9 # era None abbiamo messo 9\n","    rater_a = np.array(rater_a, dtype=int)\n","    rater_b = np.array(rater_b, dtype=int)\n","    assert(len(rater_a) == len(rater_b))\n","    if min_rating is None:\n","        min_rating = min(min(rater_a), min(rater_b))\n","    if max_rating is None:\n","        max_rating = max(max(rater_a), max(rater_b))\n","    conf_mat = Cmatrix(rater_a, rater_b,\n","                                min_rating, max_rating)\n","    num_ratings = len(conf_mat)\n","    num_scored_items = float(len(rater_a))\n","\n","    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n","    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n","\n","    numerator = 0.0\n","    denominator = 0.0\n","\n","    for i in range(num_ratings):\n","        for j in range(num_ratings):\n","            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n","                              / num_scored_items)\n","            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n","            numerator += d * conf_mat[i][j] / num_scored_items\n","            denominator += d * expected_count / num_scored_items\n","\n","    return (1.0 - numerator / denominator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jydD-lsObD4e"},"outputs":[],"source":["path_drive = '/content/drive/My Drive/'\n","path = path_drive+'ProgettoDL/'\n","\n","os.chdir(path)\n","\n","from metrics import np_quadratic_weighted_kappa, minimum_sensitivity\n","from sklearn.metrics import mean_absolute_error\n","\n","#errore no graph before run \n","tf.compat.v1.disable_eager_execution()\n","\n","\n","\n","def compute_metrics(y_true, y_pred, num_classes):\n","\n","  #run function minimum_sensitivity\n","\n","\n","  # Calculate metric\n","  sess = keras.backend.get_session()\n","\n","  #qwk = np_quadratic_weighted_kappa(np.argmax(y_true, axis=0), np.argmax(y_pred, axis=0), 0,\n","\t#\t\t\t\t\t\t\t\t\tnum_classes - 1)\n","  \n","  qwk = quadratic_weighted_kappa(y_true, y_pred)\n","  ms = minimum_sensitivity(y_test_no_argmax, y_pred_no_argmax)\n","  mae = sess.run(K.mean(keras.losses.mean_absolute_error(y_test_no_argmax, y_pred_no_argmax)))\n","  \n","  metrics = {\n","\t\t'QWK': qwk,\n","\t\t'MS': ms,\n","\t\t'MAE': mae}\n","  \n","  return metrics\n","\n","def print_metrics(metrics):\n","\tprint('QWK: {:.4f}'.format(metrics['QWK']))\n","\tprint('MS: {:.4f}'.format(metrics['MS']))\n","\tprint('MAE: {:.4f}'.format(metrics['MAE']))    \n","\n","\n","#-----codice------\n","\n","num_classi = 10\n","metrics = compute_metrics(y_test, y_pred,num_classi)\n","print_metrics(metrics)\n","\n","with open(\"metrics.txt\", \"w\") as text_file:\n","    print(print_metrics, file=text_file)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Tnlbf22bBpIt"},"source":["***Metrice Ottenute***\n","\n","**K Cohen**   https://it.vvikipedla.com/wiki/Cohen%27s_kappa\n","Il Kappa di Cohen è un coefficiente statistico che rappresenta il grado di accuratezza e affidabilità in una classificazione statistica; è un indice di concordanza che tiene conto della probabilità di concordanza casuale; l'indice calcolato in base al rapporto tra l'accordo in eccesso rispetto alla probabilità di concordanza casuale e l'eccesso massimo ottenibile. Attraverso la matrice di confusione è possibile valutare questo parametro. In particolare ... Esistono diversi \"gradi di concordanza\", in base ai quali possiamo definire se Kappa di Cohen è scarso o ottimo:\n","\n","- se k assume valori inferiori a 0, allora non c'è concordanza;\n","- se k assume valori compresi tra 0-0,4, allora la concordanza è scarsa;\n","- se k assume valori compresi tra 0,4-0,6, allora la concordanza è discreta;\n","- se k assume valori compresi tra 0,6-0,8, la concordanza è buona;\n","- se k assume valori compresi tra 0,8-1, la concordanza è ottima.\n","\n","**QWK**: 0.7849\n","\n","BLA BLA BLA \n","\n","**MS**: 1.0000\n","\n","\n","In statistics, **mean absolute error (MAE)** is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. \n","\n","**MAE**: 0.0000"]},{"cell_type":"markdown","metadata":{"id":"z25yiN0zKZ5Y"},"source":["## PLOT CONFUSION MATRIX FUNCTION "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LsYoP-3hgfY5"},"outputs":[],"source":["'''METHOD FOR PLOT CONFUSION MATRIX'''\n","#Confusion Matrix - CROP\n","import sklearn.metrics as metrics\n","\n","def plot_confusion_matrix(cm,\n","                          target_names,\n","                          title='Confusion matrix',\n","                          cmap=None,\n","                          normalize=True):\n","\n","    accuracy = np.trace(cm) / float(np.sum(cm))\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    fig = plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names, rotation=45)\n","        plt.yticks(tick_marks, target_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    for i in range (cm.shape[0]):\n","      for j in range (cm.shape[1]):\n","        if normalize:\n","            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","        else:\n","            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","    axes = plt.gca()\n","    bottom, top = axes.get_ylim()\n","    axes.set_ylim(bottom + 0.5, top - 0.5)\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","    plt.show()\n","    \n","    return fig\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"S0JFOHg2KUnT"},"source":["## PLOT CONFUSION MATRIX "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpj28QpDKTkz"},"outputs":[],"source":["'''PLOT ENTIRE CONFUSION MATRIX OF THE SUB-SET TEST'''\n","\n","import sklearn.metrics as metrics\n","cm = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)\n","fig = plot_confusion_matrix(cm,\n","                      target_names = classi,\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix IMG \")\n","plt.savefig(os.path.join(path+'weights/CM_{}_{}.pdf'.format(immgs,cnn))) \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yy4ylbat-r51"},"source":["##PLOT CONFUSION MATRIX PER CIASCUNA SERIE DEL CALCIO "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEruoZcb-q_S"},"outputs":[],"source":["'''PLOT CONFUSION MATRIX FOR EACH DISTINCT SERIES OF PRODUCTION PRESENT INTO THE SUB-SET TEST'''\n","\n","import sklearn.metrics as metrics\n","from functools import reduce\n","#serie 0\n","cm0 = metrics.confusion_matrix(y_true=test_array_s0, y_pred=pred_array_s0)\n","fig = plot_confusion_matrix(cm0,\n","                      target_names = reduce(np.union1d, (pred_array_s0, test_array_s0)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 0 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie0_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 1\n","cm1 = metrics.confusion_matrix(y_true=test_array_s1, y_pred=pred_array_s1)\n","fig = plot_confusion_matrix(cm1,\n","                      target_names = reduce(np.union1d, (pred_array_s1, test_array_s1)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 1 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie1_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 2\n","cm2 = metrics.confusion_matrix(y_true=test_array_s2, y_pred=pred_array_s2)\n","fig = plot_confusion_matrix(cm2,\n","                      target_names = reduce(np.union1d, (pred_array_s2, test_array_s2)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 2 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie2_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 3\n","cm3 = metrics.confusion_matrix(y_true=test_array_s3, y_pred=pred_array_s3)\n","fig = plot_confusion_matrix(cm3,\n","                      target_names = reduce(np.union1d, (pred_array_s3, test_array_s3)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 3 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie3_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 4\n","cm4 = metrics.confusion_matrix(y_true=test_array_s4, y_pred=pred_array_s4)\n","fig = plot_confusion_matrix(cm4,\n","                      target_names = reduce(np.union1d, (pred_array_s4, test_array_s4)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 4 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie4_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 5\n","cm5 = metrics.confusion_matrix(y_true=test_array_s5, y_pred=pred_array_s5)\n","fig = plot_confusion_matrix(cm5,\n","                      target_names = reduce(np.union1d, (pred_array_s5, test_array_s5)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 5 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie5_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 6\n","cm6 = metrics.confusion_matrix(y_true=test_array_s6, y_pred=pred_array_s6)\n","fig = plot_confusion_matrix(cm6,\n","                      target_names = reduce(np.union1d, (pred_array_s6, test_array_s6)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 6 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie6_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 7\n","cm7 = metrics.confusion_matrix(y_true=test_array_s7, y_pred=pred_array_s7)\n","fig = plot_confusion_matrix(cm7,\n","                      target_names = reduce(np.union1d, (pred_array_s7, test_array_s7)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 7 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie7_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 8\n","cm8 = metrics.confusion_matrix(y_true=test_array_s8, y_pred=pred_array_s8)\n","fig = plot_confusion_matrix(cm8,\n","                      target_names = reduce(np.union1d, (pred_array_s8, test_array_s8)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 8 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie8_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 9 \n","cm9 = metrics.confusion_matrix(y_true=test_array_s9, y_pred=pred_array_s9)\n","fig = plot_confusion_matrix(cm9,\n","                      target_names = reduce(np.union1d, (pred_array_s9, test_array_s9)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 9 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie9_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 10\n","cm10 = metrics.confusion_matrix(y_true=test_array_s10, y_pred=pred_array_s10)\n","fig = plot_confusion_matrix(cm10,\n","                      target_names = reduce(np.union1d, (pred_array_s10, test_array_s10)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 10 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie10_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 11\n","cm11 = metrics.confusion_matrix(y_true=test_array_s11, y_pred=pred_array_s11)\n","fig = plot_confusion_matrix(cm11,\n","                      target_names = reduce(np.union1d, (pred_array_s11, test_array_s11)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 11 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie11_{}_{}.pdf'.format(immgs,cnn))) \n","\n","#serie 12\n","cm12 = metrics.confusion_matrix(y_true=test_array_s12, y_pred=pred_array_s12)\n","fig = plot_confusion_matrix(cm12,\n","                      target_names = reduce(np.union1d, (pred_array_s12, test_array_s12)),\n","                      normalize    = False,\n","                      title        = \"Confusion Matrix Series 12 \")\n","plt.savefig(os.path.join(path+'weights/CM_serie12_{}_{}.pdf'.format(immgs,cnn))) "]},{"cell_type":"markdown","metadata":{"id":"TUnAYcXVclfD"},"source":["## CRAMER V CORRELATION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yqvgi3VKck3o"},"outputs":[],"source":["'''CRAMER V CORRELATION MEASUREMENT'''\n","\n","\n","'''PRIMA VERSIONE'''\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as ss\n","import seaborn as sns\n","\n","def cramers_v(confusion_matrix):\n","    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n","        uses correction from Bergsma and Wicher,\n","        Journal of the Korean Statistical Society 42 (2013): 323-328\n","    \"\"\"\n","    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n","    n = confusion_matrix.sum()\n","    phi2 = chi2 / n\n","    r, k = confusion_matrix.shape\n","    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n","    rcorr = r - ((r-1)**2)/(n-1)\n","    kcorr = k - ((k-1)**2)/(n-1)\n","    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n","\n","confusion_matrix = pd.crosstab(y_test, y_pred)\n","print(\"cramer correlation tra predizioni delle classi, e le classi effettive\")\n","cramer1 = cramers_v(confusion_matrix.values)\n","print(cramer1)\n","\n","confusion_matrix2 = pd.crosstab(y_test_series, y_pred)\n","print(\"cramer correlation tra predizioni delle classi e le ground thruth di shotgun series\")\n","cramer2 = cramers_v(confusion_matrix2.values)\n","print(cramer2)\n","\n","\n","\n","'''SECONDA VERSIONE.        https://www.youtube.com/watch?v=eTnLTJer_Oo'''\n","contTable = pd.crosstab(y_test_series, y_pred)\n","print(contTable)\n","\n","!pip install researchpy\n","\n","import researchpy\n","\n","crosstab, res = researchpy.crosstab(pd.Series(y_test_series), pd.Series(y_pred), test='chi-square')\n","print(\"\\n{}\".format(res))\n","\n","df = min(contTable.shape[0], contTable.shape[1]) - 1\n","print(\"\\ndf = {}\".format(df))\n","\n","V = res.iloc[2,1]\n","print(\"V = {}\".format(V))\n","\n","if df == 1:\n","    if V < 0.10:\n","        qual = 'negligible'\n","    elif V < 0.30:\n","        qual = 'small'\n","    elif V < 0.50:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 2:\n","    if V < 0.07:\n","        qual = 'negligible'\n","    elif V < 0.21:\n","        qual = 'small'\n","    elif V < 0.35:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 3:\n","    if V < 0.06:\n","        qual = 'negligible'\n","    elif V < 0.17:\n","        qual = 'small'\n","    elif V < 0.29:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","elif df == 4:\n","    if V < 0.05:\n","        qual = 'negligible'\n","    elif V < 0.15:\n","        qual = 'small'\n","    elif V < 0.25:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","else:\n","    if V < 0.05:\n","        qual = 'negligible'\n","    elif V < 0.13:\n","        qual = 'small'\n","    elif V < 0.22:\n","        qual = 'medium'\n","    else:\n","        qual = 'large'\n","\n","print(\"\\nquality classification of the correlation is:   {}\".format(qual))\n"]},{"cell_type":"markdown","metadata":{"id":"UHab1JLzqkKx"},"source":["To indicate the strength of the association between two nominal variables, Cramér's V (Cramér, 1946) is often used.\n","\n","As for the interpretation for Cramér's V various rules of thumb exist but one of them is from Cohen (1988, pp. 222, 224, 225) who let's the interpretation depend on the degrees of freedom, shown in the table below.\n","\n","|df*|negligible|small|medium|large|\n","|-------|---|---|---|---|\n","|1|0 < .10|.10 < .30|.30 < .50|.50 or more|\n","|2|0 < .07|.07 < .21|.21 < .35|.35 or more|\n","|3|0 < .06|.06 < .17|.17 < .29|.29 or more|\n","|4|0 < .05|.05 < .15|.15 < .25|.25 or more|\n","|5|0 < .05|.05 < .13|.13 < .22|.22 or more|\n","\n","The degrees of freedom (df*) is for Cramér's V the minimum of the number of rows, or number of columns, then minus one.\n","\n","Lets see how to obtain Cramér's V with Python, using an example.\n","\n","\n","\n","\n","**A SECONDA DEI RISULTATI E CONFRONTANDOLI CON LA TABELLA RIUSCIAMO A CAPIRE L'INTENSITA' DEL BIAS TRA DIVERSE VARIABILI**"]},{"cell_type":"markdown","metadata":{"id":"qJHn_yvTH-wH"},"source":["# METRICHE DI MISURAZIONE BIAS"]},{"cell_type":"markdown","metadata":{"id":"JuIZa1l9r0bk"},"source":["#PROVE METRICHE  (NON ORA) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgCpFJcAuiIz"},"outputs":[],"source":["from sklearn import svm\n","from sklearn.metrics import accuracy_score\n","\n","import numpy as np\n","import sklearn.datasets\n","import pandas as pd\n","import sklearn.preprocessing as preprocessing\n","from collections import namedtuple\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import StandardScaler\n","\n","'''metrics functions definition'''\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","\n","def equalized_odds_measure_TP(data, model, sensible_features, ylabel, rev_pred=1, nostro=0, y_pred= None):\n","    if nostro != 0:\n","      predictions = y_pred\n","    else:\n","      predictions = model.predict(data.data) * rev_pred\n","    \n","    truth = data.target\n","    eq_dict = {}\n","    for feature in sensible_features:\n","        eq_sensible_feature = {}\n","        values_of_sensible_feature = list(set(data.data[:, feature]))\n","        for val in values_of_sensible_feature:\n","            eq_tmp = None\n","            positive_sensitive = np.sum([1.0 if data.data[i, feature] == val and truth[i] == ylabel else 0.0\n","                                         for i in range(len(predictions))])\n","            if positive_sensitive > 0:\n","                eq_tmp = np.sum([1.0 if predictions[i] == ylabel and data.data[i, feature] == val and truth[i] == ylabel\n","                                 else 0.0 for i in range(len(predictions))]) / positive_sensitive\n","            eq_sensible_feature[val] = eq_tmp\n","        eq_dict[feature] = eq_sensible_feature\n","    return eq_dict\n","\n","\n","def equalized_odds_measure_TP_no_sensitive(data, model, sensible_features, ylabel, rev_pred=1):\n","    newdata = np.delete(data.data, sensible_features, 1)\n","    predictions = model.predict(newdata) * rev_pred\n","    truth = data.target\n","    eq_dict = {}\n","    for feature in sensible_features:\n","        eq_sensible_feature = {}\n","        values_of_sensible_feature = list(set(data.data[:, feature]))\n","        for val in values_of_sensible_feature:\n","            eq_tmp = None\n","            positive_sensitive = np.sum([1.0 if data.data[i, feature] == val and truth[i] == ylabel else 0.0\n","                                         for i in range(len(predictions))])\n","            if positive_sensitive > 0:\n","                eq_tmp = np.sum([1.0 if predictions[i] == ylabel and data.data[i, feature] == val and truth[i] == ylabel\n","                                 else 0.0 for i in range(len(predictions))]) / positive_sensitive\n","            eq_sensible_feature[val] = eq_tmp\n","        eq_dict[feature] = eq_sensible_feature\n","    return eq_dict\n","\n","\n","def equalized_odds_measure_TP_from_list_of_sensfeat(data, model, sensible_features, ylabel, rev_pred=1):\n","    predictions = model.predict(data.data) * rev_pred\n","    truth = data.target\n","    eq_dict = {}\n","    for idf, features in enumerate(sensible_features):\n","        eq_sensible_feature = {}\n","        values_of_sensible_feature = list(set(features))\n","        for val in values_of_sensible_feature:\n","            eq_tmp = None\n","            positive_sensitive = np.sum([1.0 if features[i] == val and truth[i] == ylabel else 0.0\n","                                         for i in range(len(predictions))])\n","            if positive_sensitive > 0:\n","                eq_tmp = np.sum([1.0 if predictions[i] == ylabel and features[i] == val and truth[i] == ylabel\n","                                 else 0.0 for i in range(len(predictions))]) / positive_sensitive\n","            eq_sensible_feature[val] = eq_tmp\n","        eq_dict[idf] = eq_sensible_feature\n","    return eq_dict\n","\n","\n","def equalized_odds_measure_from_pred_TP(data, pred, sensible_features, ylabel, rev_pred=1):\n","    predictions = pred * rev_pred\n","    truth = data.target\n","    eq_dict = {}\n","    for feature in sensible_features:\n","        eq_sensible_feature = {}\n","        values_of_sensible_feature = list(set(data.data[:, feature]))\n","        for val in values_of_sensible_feature:\n","            eq_tmp = None\n","            positive_sensitive = np.sum([1.0 if data.data[i, feature] == val and truth[i] == ylabel else 0.0\n","                                         for i in range(len(predictions))])\n","            if positive_sensitive > 0:\n","                eq_tmp = np.sum([1.0 if predictions[i] == ylabel and data.data[i, feature] == val and truth[i] == ylabel\n","                                 else 0.0 for i in range(len(predictions))]) / positive_sensitive\n","            eq_sensible_feature[val] = eq_tmp\n","        eq_dict[feature] = eq_sensible_feature\n","    return eq_dict\n","\n","\n","def equalized_odds_measure_FP(data, model, sensible_features, ylabel, rev_pred=1):\n","    predictions = model.predict(data.data) * rev_pred\n","    truth = data.target\n","    eq_dict = {}\n","    for feature in sensible_features:\n","        eq_sensible_feature = {}\n","        values_of_sensible_feature = list(set(data.data[:, feature]))\n","        for val in values_of_sensible_feature:\n","            eq_tmp = None\n","            positive_sensitive = np.sum([1.0 if data.data[i, feature] == val and truth[i] != ylabel else 0.0\n","                                         for i in range(len(predictions))])\n","            if positive_sensitive > 0:\n","                eq_tmp = np.sum([1.0 if predictions[i] == ylabel and data.data[i, feature] == val and truth[i] != ylabel\n","                                 else 0.0 for i in range(len(predictions))]) / positive_sensitive\n","            eq_sensible_feature[val] = eq_tmp\n","        eq_dict[feature] = eq_sensible_feature\n","    return eq_dict\n","\n","\n","# TODO: check this\n","def false_omission_rate(data, model, sensible_features, ylabel):\n","    predictions = model.predict(data.data)\n","    truth = data.target\n","    eq_dict = {}\n","    for feature in sensible_features:\n","        eq_sensible_feature = {}\n","        values_of_sensible_feature = list(set(data.data[:, feature]))\n","        for val in values_of_sensible_feature:\n","            eq_tmp = None\n","            positive_sensitive = np.sum([1.0 if data.data[i, feature] == val and truth[i] != ylabel else 0.0\n","                                         for i in range(len(predictions))])\n","            if positive_sensitive > 0:\n","                eq_tmp = np.sum([1.0 if predictions[i] == ylabel and data.data[i, feature] == val and truth[i] != ylabel\n","                                 else 0.0 for i in range(len(predictions))]) / positive_sensitive\n","            eq_sensible_feature[val] = eq_tmp\n","        eq_dict[feature] = eq_sensible_feature\n","    return eq_dict\n","\n","\n","def statistical_parity_measure(data, model, sensible_features, ylabel, nostro=0, y_pred= None):\n","    if nostro != 0:\n","      predictions = y_pred\n","    else:\n","      predictions = model.predict(data.data)\n","    #predictions = y_pred\n","    sp_dict = {}\n","    for feature in sensible_features:\n","        sp_sensible_feature = {}\n","        values_of_sensible_feature = list(set(data.data[:, feature]))\n","        for val in values_of_sensible_feature:\n","            sp_tmp = None\n","            n_sensitive = np.sum([1.0 if data.data[i, feature] == val else 0.0 for i in range(len(predictions))])\n","            if n_sensitive > 0:\n","                sp_tmp = np.sum([1.0 if predictions[i] == ylabel and data.data[i, feature] == val else 0.0\n","                                 for i in range(len(predictions))]) / n_sensitive\n","            sp_sensible_feature[val] = sp_tmp\n","        sp_dict[feature] = sp_sensible_feature\n","    return sp_dict\n","\n","\n","def disparate_impact_measure(data, model, sensible_features, nostro=0, y_pred= None):\n","    di_dict = statistical_parity_measure(data, model, sensible_features, ylabel=1, nostro = nostro, y_pred=y_pred)\n","    for feature in di_dict:\n","        values_of_sensible_feature = list(set(data.data[:, feature]))\n","        if len(values_of_sensible_feature) != 2:\n","            di_dict[feature] = {}\n","        else:\n","            di_dict[feature] = np.min([di_dict[feature][values_of_sensible_feature[0]] /\n","                                       di_dict[feature][values_of_sensible_feature[1]],\n","                                       di_dict[feature][values_of_sensible_feature[1]] /\n","                                       di_dict[feature][values_of_sensible_feature[0]]])\n","    return di_dict\n","\n","def subgrups_sensible_feature(data, sensible_feature):\n","    dict_idxs = {}\n","    values_of_sensible_feature = list(set(data.data[:, sensible_feature]))\n","    for val in values_of_sensible_feature:\n","        dict_idxs[val] = [idx for idx, x in enumerate(data.data) if x[sensible_feature] == val]\n","    return dict_idxs\n","\n","def fair_tpr_from_model(data, model, sensible_feature, nostro=0, y_pred= None):\n","    if nostro != 0:\n","      predictions = y_pred\n","    else:\n","      predictions = model.predict(data.data)\n","    truth = data.target\n","    dict_idxs = subgrups_sensible_feature(data, sensible_feature)\n","    for val in dict_idxs:\n","        dict_idxs[val] = tpr(truth[dict_idxs[val]], predictions[dict_idxs[val]])\n","    return dict_idxs\n","\n","def tpr(y_true, y_pred):\n","    if len(confusion_matrix(y_true, y_pred).ravel()) <= 3:\n","        return 1.0\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    if tp + fn == 0:\n","        return 1.0\n","    return float(tp) / float(tp + fn)\n","\n","def fpr(y_true, y_pred):\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    return float(fp) / float(fp + tn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ip8MrFQiq5RM"},"outputs":[],"source":["'''TEST BIAS METRICS CLASSIFICATIONS'''\n","\n","'''load dataset of the example code & set some attributes(target, data)'''\n","dataset = sklearn.datasets.load_diabetes()\n","# Make the target binary: high progression Vs. low progression of the disease\n","dataset.target = np.array([1 if diabetes_progression > 139 else -1 for diabetes_progression in dataset.target])\n","val0 = np.min(dataset.data[0, 1])\n","dataset.data[:, 1] = [0 if val == val0 else 1 for val in dataset.data[:, 1]]\n","diabetes = dataset\n","# 50% for train\n","ntrain = len(diabetes.target) // 2\n","\n","'''train a model to get predictions inside methods'''\n","# Train an SVM using the training set\n","clf = svm.SVC(C=10.0)\n","clf.fit(diabetes.data[:ntrain, :], diabetes.target[:ntrain])\n","# The dataset becomes the test set\n","diabetes.data = diabetes.data[ntrain:, :]\n","diabetes.target = diabetes.target[ntrain:]\n","print('-----------------MISURE REPOSITORY CON DATASET DA INTERNET-------------------')\n","# Fairness measure\n","# Accuracy\n","pred = clf.predict(diabetes.data)\n","print('Accuracy:',  accuracy_score(diabetes.target, pred))\n","print('equalized_odds_measure_TP')\n","print(equalized_odds_measure_TP(diabetes, clf, [1], ylabel=1))  \n","idxs = subgrups_sensible_feature(diabetes, sensible_feature=1)\n","print('fair_tpr_from_model')\n","print(fair_tpr_from_model(diabetes, clf, sensible_feature=1))\n","print('disparate_impact_measure')\n","print(disparate_impact_measure(diabetes, clf, sensible_features=[0,1]))\n","\n","print('-----------------MISURE NOSTRE -------------------')\n","###DATAFRAME CHE VOGLIAMO \n","###[INDICE, SERIE(variabile sensibile), CLASSE, IMG...]\n","#print('len df testgen : {} '.format(len(testgen.df)))\n","#print(testgen.df.shape)\n","#diabets_nostro.data = testgen.df.to_numpy()\n","#print('len df testgen DOPO : {} '.format(len(diabets_nostro.data)))\n","#print(diabets_nostro.data)\n","#print(diabets_nostro.data.shape)\n","#diabets_nostro.target = testgen.df['class'].to_numpy()\n","#print('target : {}'.format(diabets_nostro.target))\n","\n","from sklearn.utils import Bunch\n","diabets_nostro = Bunch(data=testgen.df.to_numpy(), target=testgen.df['class'].to_numpy())\n","\n","\n","# Accuracy\n","pred = y_pred\n","print('Accuracy (NOSTRO):',  accuracy_score(diabets_nostro.target, pred))\n","print('equalized_odds_measure_TP (NOSTRO)')\n","print(equalized_odds_measure_TP(diabets_nostro, model, [1], ylabel=[3], nostro=1, y_pred=y_pred))  \n","idxs = subgrups_sensible_feature(diabets_nostro, sensible_feature=1)\n","print('fair_tpr_from_model (NOSTRO)')\n","#print(fair_tpr_from_model(diabets_nostro, model, sensible_feature=1,nostro=1, y_pred=y_pred))\n","print('disparate_impact_measure (NOSTRO)')\n","print(disparate_impact_measure(diabets_nostro, model, sensible_features=[1], nostro=1, y_pred=y_pred)) ### non da errore, ma non da nemmeno risultati ... bisogna studiare meglio il metodo\n","\n","#diabetes.data = diabetes.data[ntrain:, :]\n","#diabetes.target = diabetes.target[ntrain:]"]},{"cell_type":"markdown","metadata":{"id":"5aSi_lI_xuvO"},"source":["# Metriche Lisa"]},{"cell_type":"markdown","metadata":{"id":"u7H7AQn_u3pW"},"source":["## Alcune Definizioni \n","*  **True Positives** (TP): Items where the true label is positive and whose class is correctly predicted to be positive.\n","*  **False Positives** (FP): Items where the true label is negative and whose class is incorrectly predicted to be positive\n","*  **True Negatives** (N): Items where the true label is negative and whose class is correctly predicted to be negative.\n","*  **False Negatives** (FN): Items where the true label is positive and whose class is incorrectly predicted to be negative.\n","\n","* **False Positive Rate**, or *Type I Error*: Number of items wrongly identified as positive out of the total actual negatives — FP/(FP+TN) - This error means that an image not containing a particular parasite egg is incorrectly labeled as having it\n","* **False Negative Rate**, or *Type II Error*: Number of items wrongly identified as negative out of the total actual positives — FN/(FN+TP). This metric is especially important to us, as it tells us the frequency with which a particular parasite egg is not classified correctly\n","\n","-------------\n","\n","* **Statistical Parity Difference**\n","This measure is based on the following formula :\n","𝑃𝑟(𝑌=1|𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑)−𝑃𝑟(𝑌=1|𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑) Here the bias or statistical imparity is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1. So it has to be close to 0 so it will be fair.\n","\n","*  **Equal Opportunity Difference** This metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula - 𝑇𝑃𝑅𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑−𝑇𝑃𝑅𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑 Same as the previous metric we need it to be close to 0.\n","\n","* **demographic parity** A fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n","\n","* **equality of opportunity** A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7dJJO7ixuJ7"},"outputs":[],"source":["!pip install fairlearn \n","from fairlearn.metrics import selection_rate\n","from fairlearn.metrics import true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate\n","from fairlearn.metrics import equalized_odds_difference\n","\n","import sklearn as sk\n","\n","\n","#---- metriche lisa ----#\n","y_true = testgen.df['class'].to_numpy()\n","SR = selection_rate(y_true, y_pred, pos_label=1, sample_weight=None)\n","print('selection_rate : {}' . format(SR))\n","\n","\n","#Per quanto riguarda AO come metrica, potremo utilizzare i risultati della confusion matrix ?\n","#LINK : https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n","#LINK : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n","#print('Unique Element Y_test : {}'.format(np.unique(y_test)))\n","#print('Unique Element Y_pred : {}'.format(np.unique(y_pred)))\n","#print('True_Positive_Rate : {}'.format(true_positive_rate(y_true, y_pred)))\n","\n","FP = cm.sum(axis=0) - np.diag(cm)  \n","FN = cm.sum(axis=1) - np.diag(cm)\n","TP = np.diag(cm)\n","TN = cm.sum() - (FP + FN + TP)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","print('TPR : {}'.format(TPR))\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","print('TNR : {}'.format(TNR))\n","# Precision or positive predictive value\n","PPV = TP/(TP+FP)\n","print('PPV : {}'.format(PPV))\n","# Negative predictive value\n","NPV = TN/(TN+FN)\n","print('NPV : {}'.format(NPV))\n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","print('FPR : {}'.format(FPR))\n","# False negative rate\n","FNR = FN/(TP+FN)\n","print('FNR : {}'.format(FNR))\n","# False discovery rate\n","FDR = FP/(TP+FP)\n","print('FDR : {}'.format(FDR))\n","\n","# Overall accuracy\n","ACC = (TP+TN)/(TP+FP+FN+TN)\n","print('Accuracy : {}'.format(ACC))\n","\n","\n","AO = 0.5*(\n","    (TPR[0] + FPR[0]) - \n","    (TPR[1] + FPR[1]) + \n","    (TPR[2] + FPR[2]) - \n","    (TPR[3] + FPR[3]) +\n","    (TPR[4] + FPR[4]) -\n","    (TPR[5] + FPR[5]) +\n","    (TPR[6] + FPR[6]) -\n","    (TPR[7] + FPR[7]) +\n","    (TPR[8] + FPR[8]) -\n","    (TPR[9] + FPR[9]))\n","\n","print('AO : {}'.format(AO))\n","#y_true= y_true.reshape(1,-1)\n","#y_pred= y_pred.reshape(-1,1)\n","#print(y_true.shape)\n","#print(y_pred.shape)\n","\n","\n","'''FORSE QUA RIUSCIAMO A TROVARE UN ESEMPIO DI APPLICAZIONE DEL METODO'''\n","'''https://deepnote.com/@Machine-Learning-2/Miniproject-z523fGqWSSu7QV34n_u7OA'''\n","'''https://fairlearn.org/main/user_guide/assessment.html'''\n","\n","\n","EO =(TPR[0] - TPR[1] + TPR[2] - TPR[3] + TPR[4] - TPR[5] + TPR[6] - TPR[7] + TPR[8] - FPR[9]) \n","print('EO : {}' . format(EO))\n","\n","\n","#Demographic parity\n","'''\n","Demographic parity is one of the most popular fairness indicators in the literature. \n","Demographic parity is achieved if the absolute number of positive predictions \n","in the subgroups are close to each other. This measure does not take true class into\n","consideration and only depends on the model predictions. In some literature, \n","demographic parity is also referred to as statsictal parity or independence.\n","'''\n","DP = (TP + FP)\n","print('Demographic parity : {}' . format(DP))\n","\n","#Equalized odds\n","'''\n","Equalized odds, also known as separation, are achieved if the sensitivities in the \n","subgroups are close to each other. The group-specific sensitivities \n","indicate the number of the true positives divided by the total \n","number of positives in that group.\n","'''\n","Equalized_Odds = TP / (TP + FN)\n","print('Equalized Odds : {}' . format(Equalized_Odds))\n","\n","\n","##---- Link Riccardo ----##\n","#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n","\n","\n","Balanced_Accuracy = sk.metrics.balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Generale : {}' . format(Balanced_Accuracy))\n","\n","\n","#####----------- PER CIASCUNA SERIE BALANCED ACCURACY -----------####\n","\n","Balanced_Accuracy_s0= sk.metrics.balanced_accuracy_score(test_array_s0, pred_array_s0, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 0 : {}' . format(Balanced_Accuracy_s0))\n","\n","Balanced_Accuracy_s1= sk.metrics.balanced_accuracy_score(test_array_s1, pred_array_s1, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 1 : {}' . format(Balanced_Accuracy_s1))\n","\n","Balanced_Accuracy_s2= sk.metrics.balanced_accuracy_score(test_array_s2, pred_array_s2, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 2 : {}' . format(Balanced_Accuracy_s2))\n","\n","Balanced_Accuracy_s3= sk.metrics.balanced_accuracy_score(test_array_s3, pred_array_s3, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 3 : {}' . format(Balanced_Accuracy_s3))\n","\n","Balanced_Accuracy_s4= sk.metrics.balanced_accuracy_score(test_array_s4, pred_array_s4, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 4 : {}' . format(Balanced_Accuracy_s4))\n","\n","Balanced_Accuracy_s5= sk.metrics.balanced_accuracy_score(test_array_s5, pred_array_s5, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 5 : {}' . format(Balanced_Accuracy_s5))\n","\n","Balanced_Accuracy_s6= sk.metrics.balanced_accuracy_score(test_array_s6, pred_array_s6, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 6 : {}' . format(Balanced_Accuracy_s6))\n","\n","Balanced_Accuracy_s7= sk.metrics.balanced_accuracy_score(test_array_s7, pred_array_s7, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 7 : {}' . format(Balanced_Accuracy_s7))\n","\n","Balanced_Accuracy_s8= sk.metrics.balanced_accuracy_score(test_array_s8, pred_array_s8, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 8 : {}' . format(Balanced_Accuracy_s8))\n","\n","Balanced_Accuracy_s9= sk.metrics.balanced_accuracy_score(test_array_s9, pred_array_s9, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 9 : {}' . format(Balanced_Accuracy_s9))\n","\n","Balanced_Accuracy_s10= sk.metrics.balanced_accuracy_score(test_array_s10, pred_array_s10, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 10 : {}' . format(Balanced_Accuracy_s10))\n","\n","Balanced_Accuracy_s11= sk.metrics.balanced_accuracy_score(test_array_s11, pred_array_s11, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 11 : {}' . format(Balanced_Accuracy_s11))\n","\n","#--------------se lavoriamo con img non croppate, manca la serie 7 mappata come serie 12 ----------------\n","#Balanced_Accuracy_s12= sk.metrics.balanced_accuracy_score(test_array_s12, pred_array_s12, sample_weight=None, adjusted=False)\n","#print('Balanced Accuracy Series 12 : {}' . format(Balanced_Accuracy_s12))\n","\n","#----------- MEDIA DELLE BALANCED ACCURACY ---------------\n","Sum = Balanced_Accuracy_s0 + Balanced_Accuracy_s1 + Balanced_Accuracy_s2 + Balanced_Accuracy_s3 + Balanced_Accuracy_s4 + Balanced_Accuracy_s5 + Balanced_Accuracy_s6 + Balanced_Accuracy_s7 + Balanced_Accuracy_s8 + Balanced_Accuracy_s9 + Balanced_Accuracy_s10 + Balanced_Accuracy_s11 #+ Balanced_Accuracy_s12 \n","Average = Sum/12\n","print('Average Balanced Accuracy : {}' . format(Average))\n"," \n","\n","##---- Wodsworth et Al ----# \n","#HIGH_RISK_GAP = SP #modulo o cardinalità \n","\n","#FN_GAP = false_negative (s1) - false negative (s2) \n","#FN_GAP = (false_negative_rate(y_true, y_pred) - false_negative_rate(y_true, y_pred))  #modulo o cardinalità\n","  \n","#FN_GAP = false_negative (s1) - false negative (s2) \n","#FP_GAP = (false_positive_rate(y_true, y_pred) - false_positive_rate(y_true, y_pred))  #modulo o cardinalità\n","\n","\n","\n","### LINK UTILE ####\n","#https://www.kaggle.com/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYiX6oW6VxPB"},"outputs":[],"source":["#---- metriche lisa ----#\n","y_true = testgen.df['class'].to_numpy()\n","SR = selection_rate(y_true, y_pred, pos_label=1, sample_weight=None)\n","print('selection_rate : {}' . format(SR))\n","\n","\n","#Per quanto riguarda AO come metrica, potremo utilizzare i risultati della confusion matrix ?\n","#LINK : https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n","#LINK : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n","#LINK : https://kozodoi.me/r/fairness/packages/2020/05/01/fairness-tutorial.html#Demographic-parity\n","#print('Unique Element Y_test : {}'.format(np.unique(y_test)))\n","#print('Unique Element Y_pred : {}'.format(np.unique(y_pred)))\n","#print('True_Positive_Rate : {}'.format(true_positive_rate(y_true, y_pred)))\n","\n","FP = cm.sum(axis=0) - np.diag(cm)  \n","FN = cm.sum(axis=1) - np.diag(cm)\n","TP = np.diag(cm)\n","TN = cm.sum() - (FP + FN + TP)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","print('TPR : {}'.format(TPR))\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","print('TNR : {}'.format(TNR))\n","# Precision or positive predictive value\n","PPV = TP/(TP+FP)\n","print('PPV : {}'.format(PPV))\n","# Negative predictive value\n","NPV = TN/(TN+FN)\n","print('NPV : {}'.format(NPV))\n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","print('FPR : {}'.format(FPR))\n","# False negative rate\n","FNR = FN/(TP+FN)\n","print('FNR : {}'.format(FNR))\n","# False discovery rate\n","FDR = FP/(TP+FP)\n","print('FDR : {}'.format(FDR))\n","\n","# Overall accuracy\n","ACC = (TP+TN)/(TP+FP+FN+TN)\n","print('Accuracy : {}'.format(ACC))\n","\n","\n","AO = 0.5*(\n","    (TPR[0] + FPR[0]) - \n","    (TPR[1] + FPR[1]) + \n","    (TPR[2] + FPR[2]) - \n","    (TPR[3] + FPR[3]) +\n","    (TPR[4] + FPR[4]) -\n","    (TPR[5] + FPR[5]) +\n","    (TPR[6] + FPR[6]) -\n","    (TPR[7] + FPR[7]) +\n","    (TPR[8] + FPR[8]) -\n","    (TPR[9] + FPR[9]))\n","\n","print('AO : {}'.format(AO))\n","#y_true= y_true.reshape(1,-1)\n","#y_pred= y_pred.reshape(-1,1)\n","#print(y_true.shape)\n","#print(y_pred.shape)\n","\n","\n","'''FORSE QUA RIUSCIAMO A TROVARE UN ESEMPIO DI APPLICAZIONE DEL METODO'''\n","'''https://deepnote.com/@Machine-Learning-2/Miniproject-z523fGqWSSu7QV34n_u7OA'''\n","'''https://fairlearn.org/main/user_guide/assessment.html'''\n","\n","\n","EO =(TPR[0] - TPR[1] + TPR[2] - TPR[3] + TPR[4] - TPR[5] + TPR[6] - TPR[7] + TPR[8] - FPR[9]) \n","print('EO : {}' . format(EO))\n","\n","\n","#Demographic parity\n","'''\n","Demographic parity is one of the most popular fairness indicators in the literature. \n","Demographic parity is achieved if the absolute number of positive predictions \n","in the subgroups are close to each other. This measure does not take true class into\n","consideration and only depends on the model predictions. In some literature, \n","demographic parity is also referred to as statsictal parity or independence.\n","'''\n","DP = (TP + FP)\n","print('Demographic parity : {}' . format(DP))\n","\n","#Equalized odds\n","'''\n","Equalized odds, also known as separation, are achieved if the sensitivities in the \n","subgroups are close to each other. The group-specific sensitivities \n","indicate the number of the true positives divided by the total \n","number of positives in that group.\n","'''\n","Equalized_Odds = TP / (TP + FN)\n","print('Equalized Odds : {}' . format(Equalized_Odds))\n"]},{"cell_type":"markdown","metadata":{"id":"GW39Ca5OZm2V"},"source":["# TSNE \n","### Spiegazioni, Link Utili e Implementazione "]},{"cell_type":"markdown","metadata":{"id":"iHlQ2bPIbBXe"},"source":["***(t-SNE)*** t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, you may have to plot fewer exploratory data analysis plots next time you work with high dimensional data.\n","\n","[Link utile ](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n","\n","***(PCA) Principal Component Analysis***\n","Lʹanalisi delle componenti principali (detta pure PCA oppure CPA) è una tecnica utilizzata nell’ambito della statistica multivariata per la semplificazione dei dati d’origine.\n","Lo scopo primario di questa tecnica è la riduzione di un numero più o meno elevato di variabili (rappresentanti altrettante caratteristiche del fenomeno analizzato) in alcune variabili latenti. Ciò avviene tramite una trasformazione lineare delle variabili che proietta quelle originarie in un nuovo sistema cartesiano nel quale le variabili vengono ordinate in ordine decrescente di varianza: pertanto, la variabile con maggiore varianza viene proiettata sul primo asse, la seconda sul secondo asse e così via. La riduzione della complessità avviene limitandosi ad analizzare le principali (per varianza) tra le nuove variabili.\n","Diversamente da altre trasformazioni (lineari) di variabili praticate nellʹambito della statistica, in questa tecnica sono gli stessi dati che determinano i vettori di trasformazione.\n","[Step By Step](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n","\n","[Link Utile](https://www.analyticsvidhya.com/blog/2020/12/an-end-to-end-comprehensive-guide-for-pca/) \n","\n","\n","***Parametri del TSNE***\n","1. **n_components** int, default=2 - Dimension of the embedded space.\n","\n","2. **perplexityfloat, default=30.0** - The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results.\n","\n","3. **early_exaggeration float, default=12.0**\n","Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.\n","\n","4. **learning_ratefloat, default=200.0** The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.\n","\n","5. **n_iterint, default=1000**\n","Maximum number of iterations for the optimization. Should be at least 250.\n","\n","6. **n_iter_without_progressint, default=300**\n","Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.\n","\n","7. **metricstr or callable, default=’euclidean’**\n","The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is “euclidean” which is interpreted as squared euclidean distance.\n","\n","8. **init{‘random’, ‘pca’} or ndarray of shape(n_samples, n_components), default=’random’**\n","Initialization of embedding. Possible options are ‘random’, ‘pca’, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.\n","\n","9. **verboseint, default=0** Verbosity level.\n","\n","10. **random_stateint, RandomState instance or None, default=None** Determines the random number generator. Pass an int for reproducible results across multiple function calls. Note that different initializations might result in different local minima of the cost function. See :term: Glossary <random_state>.\n","\n","11. **methodstr, default=’barnes_hut’**\n","By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.\n","\n","12. **n_jobsint, default=None**\n","The number of parallel jobs to run for neighbors search. This parameter has no impact when metric=\"precomputed\" or (metric=\"euclidean\" and method=\"exact\"). None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n","\n","\n","[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n","\n","[misread-tsne](https://distill.pub/2016/misread-tsne/)\n","\n","[altro modo spiegato anche meglio](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n"]},{"cell_type":"markdown","metadata":{"id":"1Y7lwf_qaa8g"},"source":["#### Calcolo TSNE & PLOT TSNE"]},{"cell_type":"markdown","metadata":{"id":"j98d2v_9aa8h"},"source":["##### TSNE QUALITY CLASS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsbiBjfuaa8h"},"outputs":[],"source":["time_start = time.time()\n","N = 1000 \n","df_subset = df.loc[rndperm[:N],:].copy()\n","data_subset = df_subset[feat_cols].values\n","#data_subset = df_subset\n","#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... originale \n","tsne = TSNE(n_components=2, verbose=1, perplexity=200, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n","tsne_results = tsne.fit_transform(data_subset)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"]},{"cell_type":"markdown","metadata":{"id":"UTnPAQncaa8i"},"source":["##### TSNE SHOTGUN SERIES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmcKJLlvaa8i"},"outputs":[],"source":["time_start = time.time()\n","N = 1000\n","df_subset_series = df_2.loc[rndperm[:N],:].copy()\n","#data_subset_series = df_subset_series\n","data_subset_series = df_subset_series[feat_cols].values\n","#tsne_series = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... \n","tsne_series = TSNE(n_components=2, verbose=1, perplexity=5, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n","tsne_results_series = tsne_series.fit_transform(data_subset_series)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"]},{"cell_type":"markdown","metadata":{"id":"hkod_FtVaa8i"},"source":["##### PLOT TSNE QUALITY CLASSES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_CnQWMV3aa8i"},"outputs":[],"source":["df_subset['tsne-2d-one'] = tsne_results[:,0]\n","df_subset['tsne-2d-two'] = tsne_results[:,1]\n","plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n","    hue=\"y\",\n","    palette=sns.color_palette('Paired', as_cmap = True),\n","    data=df_subset,\n","    legend=\"full\",\n","    alpha=0.3\n",")"]},{"cell_type":"markdown","metadata":{"id":"1hr33lawaa8i"},"source":["##### TSNE PLOT SHOTGUN SERIES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EVt__LUaa8i"},"outputs":[],"source":["df_subset_series['tsne-2d-one'] = tsne_results_series[:,0]\n","df_subset_series['tsne-2d-two'] = tsne_results_series[:,1]\n","plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n","    hue=\"y\",\n","    palette=sns.color_palette('Paired', as_cmap = True),\n","    data=df_subset_series,\n","    legend=\"full\",\n","    alpha=0.3\n",")"]},{"cell_type":"markdown","metadata":{"id":"PlveYmn8Zut-"},"source":["***(t-SNE)*** t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, you may have to plot fewer exploratory data analysis plots next time you work with high dimensional data.\n","\n","[Link utile ](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n","\n","***(PCA) Principal Component Analysis***\n","Lʹanalisi delle componenti principali (detta pure PCA oppure CPA) è una tecnica utilizzata nell’ambito della statistica multivariata per la semplificazione dei dati d’origine.\n","Lo scopo primario di questa tecnica è la riduzione di un numero più o meno elevato di variabili (rappresentanti altrettante caratteristiche del fenomeno analizzato) in alcune variabili latenti. Ciò avviene tramite una trasformazione lineare delle variabili che proietta quelle originarie in un nuovo sistema cartesiano nel quale le variabili vengono ordinate in ordine decrescente di varianza: pertanto, la variabile con maggiore varianza viene proiettata sul primo asse, la seconda sul secondo asse e così via. La riduzione della complessità avviene limitandosi ad analizzare le principali (per varianza) tra le nuove variabili.\n","Diversamente da altre trasformazioni (lineari) di variabili praticate nellʹambito della statistica, in questa tecnica sono gli stessi dati che determinano i vettori di trasformazione.\n","[Step By Step](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n","\n","[Link Utile](https://www.analyticsvidhya.com/blog/2020/12/an-end-to-end-comprehensive-guide-for-pca/) "]},{"cell_type":"markdown","metadata":{"id":"ZVq3xZNuZwRo"},"source":["***Parametri del TSNE***\n","1. **n_components** int, default=2 - Dimension of the embedded space.\n","\n","2. **perplexityfloat, default=30.0** - The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results.\n","\n","3. **early_exaggeration float, default=12.0**\n","Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.\n","\n","4. **learning_ratefloat, default=200.0** The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help.\n","\n","5. **n_iterint, default=1000**\n","Maximum number of iterations for the optimization. Should be at least 250.\n","\n","6. **n_iter_without_progressint, default=300**\n","Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50.\n","\n","7. **metricstr or callable, default=’euclidean’**\n","The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is “euclidean” which is interpreted as squared euclidean distance.\n","\n","8. **init{‘random’, ‘pca’} or ndarray of shape(n_samples, n_components), default=’random’**\n","Initialization of embedding. Possible options are ‘random’, ‘pca’, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.\n","\n","9. **verboseint, default=0** Verbosity level.\n","\n","10. **random_stateint, RandomState instance or None, default=None** Determines the random number generator. Pass an int for reproducible results across multiple function calls. Note that different initializations might result in different local minima of the cost function. See :term: Glossary <random_state>.\n","\n","11. **methodstr, default=’barnes_hut’**\n","By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.\n","\n","12. **n_jobsint, default=None**\n","The number of parallel jobs to run for neighbors search. This parameter has no impact when metric=\"precomputed\" or (metric=\"euclidean\" and method=\"exact\"). None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n","\n","\n","[scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n","\n","[misread-tsne](https://distill.pub/2016/misread-tsne/)\n","\n","[altro modo spiegato anche meglio](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)\n"]},{"cell_type":"markdown","metadata":{"id":"OGN5mYEuZ9ZT"},"source":["#### Dataset "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FaDp9iC_ZmXy"},"outputs":[],"source":["import numpy as np\n","from keras.models import Sequential\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import os\n","import pandas as pd\n","\n","os.chdir('/content/drive/MyDrive/ProgettoDL')\n","path = os.getcwd()\n","\n","col_list_sx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATOSX\", \"CLASSE_CALCIOSX\"]\n","dataframe_sx_complessivo = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_sx, sep=\";\")\n","\n","\n","col_list_dx = [\"ID\", \"COD_COMPONENTE\", \"IMG_LATODX\", \"CLASSE_CALCIODX\"]\n","dataframe_dx_complessivo = pd.read_csv(os.path.join(path + '/20201102_ExportDB.txt'), usecols=col_list_dx, sep=\";\")\n","\n","\n","dataframe_sx_complessivo.columns = ['ID','series', 'filename', 'class']\n","dataframe_dx_complessivo.columns = ['ID','series', 'filename', 'class']\n","\n","#print(dataframe_sx.columns)                 #stampo i due elementi con stesso ID (lato dx e sx di stesso CALCIO)\n","frames = [dataframe_sx_complessivo, dataframe_dx_complessivo]\n","result_complessivo = pd.concat(frames)\n","#print(result_complessivo)\n","#print(result_complessivo.loc[[1]])\n","#print(type(result_complessivo.loc[[1]]))\n","\n","result_complessivo[\"class\"] = result_complessivo[\"class\"].map({'1': int(0), '2-': int(1), '2': int(2), '2+': int(3), '3-': int(4), '3': int(5), '3+': int(6), '4-': int(7), '4': int(8), '4+': int(9)})\n","result_complessivo[\"series\"] = result_complessivo[\"series\"].map({2: int(0), 4: int(1), 8: int(2), 10: int(3), 6: int(4), 9: int(5), 3: int(6), 11: int(7), 12: int(8), 13: int(9), 14: int(10), 15: int(11), 7: int(12)})\n","\n","#IDENTIFICAZIONE VALORI NULL \n","print(\"Null VALUE di class : \"+format(result_complessivo['class'].isnull().sum()))\n","print(result_complessivo.loc[result_complessivo['class'] == '0'])\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","result_complessivo['class'] = pd.to_numeric(result_complessivo['class'], errors='coerce')\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","result_complessivo = result_complessivo.dropna(subset=['class'])    #rimuovo le righe con elementi nulli\n","print(result_complessivo[result_complessivo['class'].isnull()])\n","\n","print(\"Null VALUE di class : \"+format(result_complessivo['class'].isnull().sum()))\n","\n","#IMMG EXIST ?  (cerco se qualche path non esiste e lo elimino dal dataframe) e se esiste ne faccio la MASCHERA\n","import os.path\n","from os import path\n","os.chdir('/content/drive/MyDrive/CALCIO_NOPRE')\n","for index, row in result_complessivo.iterrows():\n","    filename = row['filename']\n","    if(os.path.exists(filename) == False):\n","      result_complessivo = result_complessivo.drop(result_complessivo[(result_complessivo['filename'] == filename)].index)\n","      print('File : {} eliminato'.format(filename))\n","\n","print('------------------- DATASET BASE ---------------')\n","print(type(result_complessivo))  \n","print(len(result_complessivo))\n","print(result_complessivo)\n","\n","result_complessivo_totale = pd.DataFrame()\n","\n","for index, row in result_complessivo.iterrows():\n","  filename_mask = 'mask_{}'.format(row['filename'])\n","  #filename_gray = 'gray_{}'.format(row['filename'])\n","  class_ = row['class']\n","  series_ = row['series']\n","  #print('{}_{}_{}_{}'.format(filename_gray,filename_mask, class_, series_)) \"ID\": row['ID']\n","  row_df_1 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename_mask, \"class\" : class_},index=[0])\n","  #row_df_2 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename_gray, \"class\" : class_},index=[0])\n","  #row_df_3 = pd.DataFrame({\"ID\": row['ID'], \"series\" : series_, \"filename\" : filename, \"class\" : class_},index=[0])\n","  #print(row_df_1)\n","  #print(row_df_2)\n","  result_complessivo_totale = result_complessivo_totale.append(row_df_1)\n","  #result_complessivo_totale = result_complessivo_totale.append(row_df_2)\n","  #result_complessivo_totale = result_complessivo_totale.append(row_df_3)\n","\n","\n","print('------------------- DATASET COMPLESSIVO ---------------') \n","print(type(result_complessivo_totale))  \n","print(len(result_complessivo_totale))\n","#print(result_complessivo_totale)\n","\n","from sklearn.utils import shuffle\n","result_complessivo_totale = shuffle(result_complessivo_totale)\n","print(type(result_complessivo_totale))  \n","print(len(result_complessivo_totale))\n","print(result_complessivo_totale)\n"]},{"cell_type":"markdown","metadata":{"id":"_U2UEFjs1iXp"},"source":["#### import utili per il TSNE e PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvdYrmbf1aYd"},"outputs":[],"source":["%matplotlib inline\n","from __future__ import print_function\n","import time\n","import numpy as np\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import seaborn as sns\n","from sklearn.manifold import TSNE\n","import pandas as pd    \n","from sklearn.preprocessing import StandardScaler\n"]},{"cell_type":"markdown","metadata":{"id":"z56gEZfX14mX"},"source":["#### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini"]},{"cell_type":"markdown","metadata":{"id":"7qiUkpP0_LXM"},"source":["##### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini - QUALITY CLASS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7t1a9DA0wnE"},"outputs":[],"source":["# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n","from tqdm import tqdm\n","immg_rows = 270 \n","immg_cols = 470\n","X = [] \n","imgs_array_tot = []\n","\n","data_X = result_complessivo_totale['filename'][:1000] #---versione originale \n","result_complessivo_totale_min = result_complessivo_totale[:1000] #--deve essere uguale a y_dim ---versione originale \n","y = result_complessivo_totale['class'][:1000] #--- deve essere uguale ... ---versione originale \n","\n","for index, row in tqdm(result_complessivo_totale_min.iterrows()):\n","    filename = row['filename']\n","    if(filename[0] == 'm'):\n","      image = load_img('/content/drive/My Drive/MASK_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    elif(filename[0] == 'g'): \n","      image = load_img('/content/drive/My Drive/GRAY_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    else:\n","      image = load_img('/content/drive/My Drive/CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    \n","    #print('Originale : {} x {} x {}'.format(image.size[0], image.size[1], len(image.size)-1))\n","    #plt.imshow(image)\n","    scale_percent = 90 # percent of original size\n","    width, height = image.size\n","    #print('channel : {}'.format(len(image.size)))\n","    width = int(width * scale_percent / 100)\n","    height = int(height * scale_percent / 100)\n","    dim = (width, height)\n","    # resize image\n","    x = img_to_array(image)\n","    resized = cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n","    #print('Ridimensionata : {}'.format((resized.shape)))\n","    #print('Resized Dimensions : ',resized.shape)\n","    imgs_array_tot.append(resized)\n","    X = np.asarray(imgs_array_tot)\n","print(X.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"7IsDgmRB_QxY"},"source":["##### IMG to ARRAY per il calcolo del PCA e TSNE & Reduction delle immagini - SHOTGUN SERIES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WGaPWme_Ca8"},"outputs":[],"source":["# https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n","from tqdm import tqdm\n","\n","immg_rows = 270 \n","immg_cols = 470\n","X = [] \n","imgs_array_tot = []\n","data_X = result_complessivo_totale['filename'][:1000]\n","\n","result_complessivo_totale_min = result_complessivo_totale[:1000] #--deve essere uguale a y_dim\n","\n","y = result_complessivo_totale['class'][:1000] #--- deve essere uguale ... \n","\n","y_series = result_complessivo_totale['series'][:1000] #--- deve essere uguale ...\n","\n","for index, row in tqdm(result_complessivo_totale_min.iterrows()):\n","    filename = row['filename']\n","    if(filename[0] == 'm'):\n","      image = load_img('/content/drive/My Drive/MASK_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    elif(filename[0] == 'g'): \n","      image = load_img('/content/drive/My Drive/GRAY_CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","    else:\n","      image = load_img('/content/drive/My Drive/CALCIO_CROP/{}'.format(filename), target_size = (immg_rows, immg_cols, 1), color_mode=\"grayscale\")\n","\n","    scale_percent = 90 # percent of original size\n","    width, height = image.size\n","    width = int(width * scale_percent / 100)\n","    height = int(height * scale_percent / 100)\n","    dim = (width, height)\n","    # resize image\n","    x = img_to_array(image)\n","    resized = cv2.resize(x, dim, interpolation = cv2.INTER_AREA)\n","    #print('Ridimensionata : {}'.format((resized.shape)))\n","    #print('Resized Dimensions : ',resized.shape)\n","    imgs_array_tot.append(resized)\n","    X2 = np.asarray(imgs_array_tot)\n","print(X2.shape) "]},{"cell_type":"markdown","metadata":{"id":"ihY4u-CL2JKI"},"source":["#### Check & Create Dataframe for PCA (Principal Analysis Component) & T-SNE (t-distributed stochastic neighbor embedding)"]},{"cell_type":"markdown","metadata":{"id":"2OaloopJ5gCK"},"source":["##### classi di qualità "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niWaWydy2H9P"},"outputs":[],"source":["print('X SHAPE : {}'.format(X.shape))\n","\n","nsamples = X.shape[0]\n","rows = X.shape[1]\n","cols = X.shape[2]\n","channel = 1\n","\n","print('n_samples : {} , rows : {} , cols : {} , channel : {} '.format(nsamples, rows, cols, channel))\n","print(type(X))\n","X_1 = np.reshape(X, (X.shape[0],rows*cols*channel)) #-- serve per modificare la dimensione, per il fit_transform          FORSE QUI BISOGNA SOLO USARE I PRIMI 2 VALORI E IL 3 DEI CANALI NO!\n","\n","print('X MODIFICATO : {}'.format(X_1.shape)) #--- controllo se ho fatto tutto correttamente \n","\n","feat_cols = [ 'pixel'+str(i) for i in range(X_1.shape[1]) ]\n","print('Feat Cols : {} '.format(len(feat_cols)))\n","#print(feat_cols)\n","df = pd.DataFrame(X_1,columns=feat_cols)\n","#df = pd.DataFrame(X_1)\n","df['y'] = pd.DataFrame({ 'y': np.array(y) })\n","df['label'] = df['y'].apply(lambda i: str(i))\n","#X, y = None, None\n","print('Size of the dataframe: {}'.format(df.shape))\n","\n","# For reproducability of the results\n","np.random.seed(42)\n","rndperm = np.random.permutation(df.shape[0])\n"]},{"cell_type":"markdown","metadata":{"id":"qrtXkNtk5luP"},"source":["##### shotgun series "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nRob1Q0F5buA"},"outputs":[],"source":["print('X2 SHAPE : {}'.format(X2.shape))\n","\n","nsamples = X2.shape[0]\n","rows = X2.shape[1]\n","cols = X2.shape[2]\n","channel = 1\n","\n","print('n_samples : {} , rows : {} , cols : {} , channel : {} '.format(nsamples, rows, cols, channel))\n","print(type(X2))\n","X_11 = np.reshape(X2, (X2.shape[0],rows*cols*channel)) #-- serve per modificare la dimensione, per il fit_transform          FORSE QUI BISOGNA SOLO USARE I PRIMI 2 VALORI E IL 3 DEI CANALI NO!\n","\n","print('X MODIFICATO : {}'.format(X_11.shape)) #--- controllo se ho fatto tutto correttamente \n","#print(X_1)\n","\n","feat_cols = [ 'pixel'+str(i) for i in range(X_11.shape[1]) ]\n","print('Feat Cols : {} '.format(len(feat_cols)))\n","#print(feat_cols)\n","df_2 = pd.DataFrame(X_11,columns=feat_cols)\n","#df = pd.DataFrame(X_1)\n","df_2['y'] = pd.DataFrame({ 'y': np.array(y_series) })\n","df_2['label'] = df_2['y'].apply(lambda i: str(i))\n","#X, y = None, None\n","print('Size of the dataframe: {}'.format(df_2.shape))\n","\n","\n","\n","# For reproducability of the results\n","np.random.seed(42)\n","rndperm = np.random.permutation(df_2.shape[0])\n"]},{"cell_type":"markdown","metadata":{"id":"hbq6tUyY2bxr"},"source":["#### Calcolo TSNE & PLOT TSNE"]},{"cell_type":"markdown","metadata":{"id":"PUPCoiGQ6j-D"},"source":["##### TSNE QUALITY CLASS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bg-1zy_g2bDc"},"outputs":[],"source":["time_start = time.time()\n","N = 1000 \n","df_subset = df.loc[rndperm[:N],:].copy()\n","data_subset = df_subset[feat_cols].values\n","#data_subset = df_subset\n","#tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... originale \n","tsne = TSNE(n_components=2, verbose=1, perplexity=200, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n","tsne_results = tsne.fit_transform(data_subset)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"]},{"cell_type":"markdown","metadata":{"id":"J9nnpLbE6sfN"},"source":["##### TSNE SHOTGUN SERIES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5Ht1Qoc6rtg"},"outputs":[],"source":["time_start = time.time()\n","N = 1000\n","df_subset_series = df_2.loc[rndperm[:N],:].copy()\n","#data_subset_series = df_subset_series\n","data_subset_series = df_subset_series[feat_cols].values\n","#tsne_series = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=3000, init='random', n_jobs = 10) #-- non so se serve init ... \n","tsne_series = TSNE(n_components=2, verbose=1, perplexity=5, n_iter=6000, init='random', n_jobs = 10) #-- nuova versione \n","tsne_results_series = tsne_series.fit_transform(data_subset_series)\n","print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"]},{"cell_type":"markdown","metadata":{"id":"RdicUGi28Sr_"},"source":["##### PLOT TSNE QUALITY CLASSES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXkTZdoA3Thu"},"outputs":[],"source":["df_subset['tsne-2d-one'] = tsne_results[:,0]\n","df_subset['tsne-2d-two'] = tsne_results[:,1]\n","plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n","    hue=\"y\",\n","    palette=sns.color_palette('Paired', as_cmap = True),\n","    data=df_subset,\n","    legend=\"full\",\n","    alpha=0.3\n",")"]},{"cell_type":"markdown","metadata":{"id":"1WOsA8la8b6O"},"source":["##### TSNE PLOT SHOTGUN SERIES "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOdq6FA08f4b"},"outputs":[],"source":["df_subset_series['tsne-2d-one'] = tsne_results_series[:,0]\n","df_subset_series['tsne-2d-two'] = tsne_results_series[:,1]\n","plt.figure(figsize=(16,10))\n","sns.scatterplot(\n","    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n","    hue=\"y\",\n","    palette=sns.color_palette('Paired', as_cmap = True),\n","    data=df_subset_series,\n","    legend=\"full\",\n","    alpha=0.3\n",")"]},{"cell_type":"markdown","metadata":{"id":"o9NFVVQUa1aY"},"source":["# **Metriche Nuove**"]},{"cell_type":"markdown","metadata":{"id":"_i67wapQa1aY"},"source":["## Alcune Definizioni \n","*  **True Positives** (TP): Items where the true label is positive and whose class is correctly predicted to be positive.\n","*  **False Positives** (FP): Items where the true label is negative and whose class is incorrectly predicted to be positive\n","*  **True Negatives** (N): Items where the true label is negative and whose class is correctly predicted to be negative.\n","*  **False Negatives** (FN): Items where the true label is positive and whose class is incorrectly predicted to be negative.\n","\n","* **False Positive Rate**, or *Type I Error*: Number of items wrongly identified as positive out of the total actual negatives — FP/(FP+TN) - This error means that an image not containing a particular parasite egg is incorrectly labeled as having it\n","* **False Negative Rate**, or *Type II Error*: Number of items wrongly identified as negative out of the total actual positives — FN/(FN+TP). This metric is especially important to us, as it tells us the frequency with which a particular parasite egg is not classified correctly\n","\n","-------------\n","\n","* **Statistical Parity Difference**\n","This measure is based on the following formula :\n","𝑃𝑟(𝑌=1|𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑)−𝑃𝑟(𝑌=1|𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑) Here the bias or statistical imparity is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1. So it has to be close to 0 so it will be fair.\n","\n","*  **Equal Opportunity Difference** This metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula - 𝑇𝑃𝑅𝐷=𝑢𝑛𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑−𝑇𝑃𝑅𝐷=𝑝𝑟𝑖𝑣𝑖𝑙𝑒𝑔𝑒𝑑 Same as the previous metric we need it to be close to 0.\n","\n","* **demographic parity** A fairness metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n","\n","* **equality of opportunity** A fairness metric that checks whether, for a preferred label (one that confers an advantage or benefit to a person) and a given attribute, a classifier predicts that preferred label equally well for all values of that attribute. In other words, equality of opportunity measures whether the people who should qualify for an opportunity are equally likely to do so regardless of their group membership."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4W7JU3l7a1aZ"},"outputs":[],"source":["!pip install fairlearn \n","from fairlearn.metrics import selection_rate\n","from fairlearn.metrics import true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate\n","from fairlearn.metrics import equalized_odds_difference\n","\n","import sklearn as sk\n","\n","\n","#---- metriche lisa ----#\n","y_true = testgen.df['class'].to_numpy()\n","SR = selection_rate(y_true, y_pred, pos_label=1, sample_weight=None)\n","print('selection_rate : {}' . format(SR))\n","\n","\n","#Per quanto riguarda AO come metrica, potremo utilizzare i risultati della confusion matrix ?\n","#LINK : https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n","#LINK : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix\n","#print('Unique Element Y_test : {}'.format(np.unique(y_test)))\n","#print('Unique Element Y_pred : {}'.format(np.unique(y_pred)))\n","#print('True_Positive_Rate : {}'.format(true_positive_rate(y_true, y_pred)))\n","\n","FP = cm.sum(axis=0) - np.diag(cm)  \n","FN = cm.sum(axis=1) - np.diag(cm)\n","TP = np.diag(cm)\n","TN = cm.sum() - (FP + FN + TP)\n","\n","# Sensitivity, hit rate, recall, or true positive rate\n","TPR = TP/(TP+FN)\n","print('TPR : {}'.format(TPR))\n","# Specificity or true negative rate\n","TNR = TN/(TN+FP) \n","print('TNR : {}'.format(TNR))\n","# Precision or positive predictive value\n","PPV = TP/(TP+FP)\n","print('PPV : {}'.format(PPV))\n","# Negative predictive value\n","NPV = TN/(TN+FN)\n","print('NPV : {}'.format(NPV))\n","# Fall out or false positive rate\n","FPR = FP/(FP+TN)\n","print('FPR : {}'.format(FPR))\n","# False negative rate\n","FNR = FN/(TP+FN)\n","print('FNR : {}'.format(FNR))\n","# False discovery rate\n","FDR = FP/(TP+FP)\n","print('FDR : {}'.format(FDR))\n","\n","# Overall accuracy\n","ACC = (TP+TN)/(TP+FP+FN+TN)\n","print('Accuracy : {}'.format(ACC))\n","\n","\n","AO = 0.5*(\n","    (TPR[0] + FPR[0]) - \n","    (TPR[1] + FPR[1]) + \n","    (TPR[2] + FPR[2]) - \n","    (TPR[3] + FPR[3]) +\n","    (TPR[4] + FPR[4]) -\n","    (TPR[5] + FPR[5]) +\n","    (TPR[6] + FPR[6]) -\n","    (TPR[7] + FPR[7]) +\n","    (TPR[8] + FPR[8]) -\n","    (TPR[9] + FPR[9]))\n","\n","print('AO : {}'.format(AO))\n","#y_true= y_true.reshape(1,-1)\n","#y_pred= y_pred.reshape(-1,1)\n","#print(y_true.shape)\n","#print(y_pred.shape)\n","\n","\n","'''FORSE QUA RIUSCIAMO A TROVARE UN ESEMPIO DI APPLICAZIONE DEL METODO'''\n","'''https://deepnote.com/@Machine-Learning-2/Miniproject-z523fGqWSSu7QV34n_u7OA'''\n","'''https://fairlearn.org/main/user_guide/assessment.html'''\n","\n","\n","EO =(TPR[0] - TPR[1] + TPR[2] - TPR[3] + TPR[4] - TPR[5] + TPR[6] - TPR[7] + TPR[8] - FPR[9]) \n","print('EO : {}' . format(EO))\n","\n","\n","#Demographic parity\n","'''\n","Demographic parity is one of the most popular fairness indicators in the literature. \n","Demographic parity is achieved if the absolute number of positive predictions \n","in the subgroups are close to each other. This measure does not take true class into\n","consideration and only depends on the model predictions. In some literature, \n","demographic parity is also referred to as statsictal parity or independence.\n","'''\n","DP = (TP + FP)\n","print('Demographic parity : {}' . format(DP))\n","\n","#Equalized odds\n","'''\n","Equalized odds, also known as separation, are achieved if the sensitivities in the \n","subgroups are close to each other. The group-specific sensitivities \n","indicate the number of the true positives divided by the total \n","number of positives in that group.\n","'''\n","Equalized_Odds = TP / (TP + FN)\n","print('Equalized Odds : {}' . format(Equalized_Odds))\n","\n","\n","##---- Link Riccardo ----##\n","#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n","\n","\n","Balanced_Accuracy = sk.metrics.balanced_accuracy_score(y_true, y_pred, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Generale : {}' . format(Balanced_Accuracy))\n","\n","\n","#####----------- PER CIASCUNA SERIE BALANCED ACCURACY -----------####\n","\n","Balanced_Accuracy_s0= sk.metrics.balanced_accuracy_score(test_array_s0, pred_array_s0, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 0 : {}' . format(Balanced_Accuracy_s0))\n","\n","Balanced_Accuracy_s1= sk.metrics.balanced_accuracy_score(test_array_s1, pred_array_s1, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 1 : {}' . format(Balanced_Accuracy_s1))\n","\n","Balanced_Accuracy_s2= sk.metrics.balanced_accuracy_score(test_array_s2, pred_array_s2, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 2 : {}' . format(Balanced_Accuracy_s2))\n","\n","Balanced_Accuracy_s3= sk.metrics.balanced_accuracy_score(test_array_s3, pred_array_s3, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 3 : {}' . format(Balanced_Accuracy_s3))\n","\n","Balanced_Accuracy_s4= sk.metrics.balanced_accuracy_score(test_array_s4, pred_array_s4, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 4 : {}' . format(Balanced_Accuracy_s4))\n","\n","Balanced_Accuracy_s5= sk.metrics.balanced_accuracy_score(test_array_s5, pred_array_s5, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 5 : {}' . format(Balanced_Accuracy_s5))\n","\n","Balanced_Accuracy_s6= sk.metrics.balanced_accuracy_score(test_array_s6, pred_array_s6, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 6 : {}' . format(Balanced_Accuracy_s6))\n","\n","Balanced_Accuracy_s7= sk.metrics.balanced_accuracy_score(test_array_s7, pred_array_s7, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 7 : {}' . format(Balanced_Accuracy_s7))\n","\n","Balanced_Accuracy_s8= sk.metrics.balanced_accuracy_score(test_array_s8, pred_array_s8, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 8 : {}' . format(Balanced_Accuracy_s8))\n","\n","Balanced_Accuracy_s9= sk.metrics.balanced_accuracy_score(test_array_s9, pred_array_s9, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 9 : {}' . format(Balanced_Accuracy_s9))\n","\n","Balanced_Accuracy_s10= sk.metrics.balanced_accuracy_score(test_array_s10, pred_array_s10, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 10 : {}' . format(Balanced_Accuracy_s10))\n","\n","Balanced_Accuracy_s11= sk.metrics.balanced_accuracy_score(test_array_s11, pred_array_s11, sample_weight=None, adjusted=False)\n","print('Balanced Accuracy Series 11 : {}' . format(Balanced_Accuracy_s11))\n","\n","#non è presente nel caso di img non crop \n","#Balanced_Accuracy_s12= sk.metrics.balanced_accuracy_score(test_array_s12, pred_array_s12, sample_weight=None, adjusted=False)\n","#print('Balanced Accuracy Series 12 : {}' . format(Balanced_Accuracy_s12))\n","\n","#----------- MEDIA DELLE BALANCED ACCURACY ---------------\n","Sum = Balanced_Accuracy_s0 + Balanced_Accuracy_s1 + Balanced_Accuracy_s2 + Balanced_Accuracy_s3 + Balanced_Accuracy_s4 + Balanced_Accuracy_s5 + Balanced_Accuracy_s6 + Balanced_Accuracy_s7 + Balanced_Accuracy_s8 + Balanced_Accuracy_s9 + Balanced_Accuracy_s10 + Balanced_Accuracy_s11 #+ Balanced_Accuracy_s12 \n","Average = Sum/12\n","print('Average Balanced Accuracy : {}' . format(Average))\n"," \n","\n","##---- Wodsworth et Al ----# \n","#HIGH_RISK_GAP = SP #modulo o cardinalità \n","\n","#FN_GAP = false_negative (s1) - false negative (s2) \n","#FN_GAP = (false_negative_rate(y_true, y_pred) - false_negative_rate(y_true, y_pred))  #modulo o cardinalità\n","  \n","#FN_GAP = false_negative (s1) - false negative (s2) \n","#FP_GAP = (false_positive_rate(y_true, y_pred) - false_positive_rate(y_true, y_pred))  #modulo o cardinalità\n","\n","\n","\n","### LINK UTILE ####\n","#https://www.kaggle.com/nathanlauga/ethics-and-ai-how-to-prevent-bias-on-ml"]},{"cell_type":"markdown","metadata":{"id":"UblH1rJGa1aa"},"source":["##PROVA MDSS (ATTUALMENTE NON FUNZIONA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtoP7blPa1aa"},"outputs":[],"source":["'''imports'''\n","'''Importing packages & libraries for Bias metrics from AIF360 (end page analysis)'''\n","\n","\"\"\"\n","import sys\n","import itertools\n","import datetime\n","\n","### altri package necessari \n","!pip install tempeh -q --force-reinstall\n","!pip install fairlearn -q --force-reinstall\n","!pip install GitPython -q --force-reinstall\n","\n","#download aif360 \n","#!pip install -r requirement.txt \n","#!python setup.py\n","###test  errore nella foto \n","\n","#!pip uninstall scikit-learn -q -y \n","# errore cercato in rete : https://github.com/pycaret/pycaret/issues/704\n","# INSTALL CONDA ON GOOGLE COLAB\n","#! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n","#! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n","#! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n","#sys.path.append('/usr/local/lib/python3.7/site-packages/')\n","# INSTALL PACKAGE CON CONDA \n","#!conda install -c conda-forge scikit-learn -y \n","\n","## package necessario utile per il gitclone\n","from git import Repo\n","\n","date_ = datetime.datetime.now()\n","#Repo.clone_from(\"link ... .git\", \"path di salvataggio\")\n","#Repo.clone_from(\"https://github.com/Trusted-AI/AIF360.git\", \"/content/aif360_repo_{}\".format(date_))\n","Repo.clone_from(\"https://github.com/Trusted-AI/AIF360.git\", \"/content/aif360\") #controllare sempre, se è già salvato ... se salvato basta commentarlo\n","\n","#---serve per aggiungere un nuovo package alla lista dei package installati\n","sys.path.append('/content/aif360/')\n","\n","\n","from aif360.metrics.mdss.ScoringFunctions import Bernoulli, ScoringFunction \n","from aif360.metrics.mdss import MDSS\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import OrdinalEncoder\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score,classification_report, f1_score, precision_score, recall_score, confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","\n","from IPython.display import Markdown, display\n","import numpy as np\n","import pandas as pd\n","\n","from collections import defaultdict\n","from aif360.datasets import BinaryLabelDataset\n","from aif360.metrics import ClassificationMetric\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JvYbNfBa1aa"},"outputs":[],"source":["\"\"\"from aif360.datasets import StandardDataset\n","dataset = StandardDataset(df, label_name='two_year_recid', favorable_classes=[0],\n","                 protected_attribute_names=['sex', 'race'],\n","                 privileged_classes=[[1], [1]],\n","                 instance_weights_name=None)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHmSazzsa1aa"},"outputs":[],"source":["\"\"\"\n","# !python3 -m pip install <pkg> -q il -q è per il quiet (per non avere quel macello in output) --force-reinstall\n","\n","#https://github.com/Trusted-AI/AIF360\n","'''REPOSITORY PRINCIPALE DOVE SI TROVANO TUTTE LE RESTANTI CARTELLE E IMPLEMENTAZIONI'''\n","#https://github.com/Trusted-AI/AIF360/blob/master/examples/demo_mdss_classifier_metric.ipynb\n","'''file dove il tipo fa un esempio di uso di questa metrica!'''\n","#https://aif360.readthedocs.io/en/latest/index.html\n","'''link di aif360 dove ci sono i metodi CHE IMPORTA SOTTO'''\n","\n","\n","class MDSSClassificationMetric(ClassificationMetric):\n","    '''\n","        Bias subset scanning is proposed as a technique to identify bias in predictive models using subset scanning [1].\n","        This class is a wrapper for the bias scan scoring and scanning methods that uses the ClassificationMetric abstraction.\n","    References:\n","        .. [1] Zhang, Z., & Neill, D. B. (2016). Identifying significant predictive bias in classifiers. arXiv preprint arXiv:1611.08292.\n","    '''\n","    def __init__(self, dataset: BinaryLabelDataset, classified_dataset: BinaryLabelDataset, \n","                scoring_function: ScoringFunction = Bernoulli(direction='positive'), unprivileged_groups: dict = None, privileged_groups:dict = None):\n","    \n","        super(MDSSClassificationMetric, self).__init__(dataset, classified_dataset,\n","                                                       unprivileged_groups=unprivileged_groups,\n","                                                       privileged_groups=privileged_groups)\n","        \n","        self.scanner = MDSS(scoring_function)\n","    \n","    def score_groups(self, privileged=True, penalty = 1e-17):\n","        '''\n","        compute the bias score for a prespecified group of records.\n","        \n","        :param privileged: flag for group to score - privileged group (True) or unprivileged group (False).\n","        This abstract the need to explicitly specify the direction of bias to scan for which depends on what the favourable label is.\n","        :param penalty: penalty term. Should be positive. The penalty term as with any regularization parameter may need to be \n","        tuned for ones use case. The higher the penalty, the less complex (number of features and feature values) the highest scoring\n","        subset that gets returned is.\n","        \n","        :returns: the score for the group\n","        '''\n","        groups = self.privileged_groups if privileged else self.unprivileged_groups\n","        subset = dict()\n","        \n","        xor_op = privileged ^ bool(self.classified_dataset.favorable_label)\n","        direction = 'positive' if xor_op else 'negative'\n","\n","        for g in groups:\n","            for k, v in g.items():\n","                if k in subset.keys():\n","                    subset[k].append(v)\n","                else:\n","                    subset[k] = [v]\n","        \n","        coordinates = pd.DataFrame(self.dataset.features, columns=self.dataset.feature_names)\n","        expected = pd.Series(self.classified_dataset.scores.flatten())\n","        outcomes = pd.Series(self.dataset.labels.flatten())\n","        \n","        self.scanner.scoring_function.kwargs['direction'] = direction\n","        return self.scanner.score_current_subset(coordinates, expected, outcomes, dict(subset), penalty)\n","    \n","    def bias_scan(self, privileged=True, num_iters = 10, penalty = 1e-17):\n","        '''\n","        scan to find the highest scoring subset of records\n","        \n","        :param privileged: flag for group to scan for - privileged group (True) or unprivileged group (False). \n","        This abstract the need to explicitly specify the direction of bias to scan for which depends on what the favourable label is.\n","        :param num_iters: number of iterations (random restarts)\n","        :param penalty: penalty term. Should be positive. The penalty term as with any regularization parameter may need to be \n","        tuned for ones use case. The higher the penalty, the less complex (number of features and feature values) the highest scoring\n","        subset that gets returned is.\n","        \n","        :returns: the highest scoring subset and the score\n","        '''\n","\n","        xor_op = privileged ^ bool(self.classified_dataset.favorable_label)\n","        direction = 'positive' if xor_op else 'negative'\n","        self.scanner.scoring_function.kwargs['direction'] = direction\n","\n","        coordinates = pd.DataFrame(self.classified_dataset.features, columns=self.classified_dataset.feature_names)\n","        \n","        expected = pd.Series(self.classified_dataset.scores.flatten())\n","        outcomes = pd.Series(self.dataset.labels.flatten())\n","        \n","        return self.scanner.scan(coordinates, expected, outcomes, penalty, num_iters)\n","\n","from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_compas\n","\n","#import requests\n","#url = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n","#r = requests.get(url, allow_redirects=True)\n","#open('dataset.csv', 'wb').write(r.content)\n","#dataset_orig = pd.read_csv('/content/dataset.csv', sep=\",\")  \n","\n","\n","series_group = [{'series': 0, 'series': 1, 'series': 2, 'series': 3, 'series': 4, 'series': 5, 'series': 6, 'series': 7, 'series': 8, 'series': 9, 'series': 10, 'series': 11, 'series': 12}]\n","classes_group = [{'class': 0, 'class': 1, 'class': 2, 'class': 3, 'class': 4, 'class': 5, 'class': 6, 'class': 7, 'class': 8, 'class': 9}]\n","\n","\"\"\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Hq8CiQka1ab"},"outputs":[],"source":["\"\"\"\n","\n","mdss_classified = MDSSClassificationMetric(dataset_orig_test, dataset_bias_test,\n","                         unprivileged_groups=male_group,\n","                         privileged_groups=female_group)\n","\n","\n","mdss_classified = MDSSClassificationMetric(test_balance_df, dataset_bias_test,\n","                         unprivileged_groups=classes_group,\n","                         privileged_groups=series_group)\n","\n","series_privileged_score = mdss_classified.score_groups(privileged=True)\n","print(series_privileged_score)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gd8PNQF-7Bi_"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ox6IFmrBfpP-","YEzVkjhHbISE","xuS19wSXbdE8","G4OSrOanfe7I","Xzh70UKnU-fM","z0DHg_Q20wBX","28i0TbSpzt_Z"],"machine_shape":"hm","name":"ALEX & DENIS - FUNZIONANTE - adversarial-debiasing.ipynb","provenance":[{"file_id":"1e4Y9JykKQU4LTHcLHy5sAzpwI-SPmTl6","timestamp":1623766889995},{"file_id":"1d0RkIbwRSJuanJjnXiLFbQSY2wZpHWXW","timestamp":1623496078537},{"file_id":"1eCj3WI6GCKlwC6VldMXH9RiHM1W8W5kj","timestamp":1623321103095},{"file_id":"1tcILVHrFk_-CCLnWgai88ENO5EJx1w_0","timestamp":1623062215958},{"file_id":"1GtF5i2sJMGQwC1uEVhcJJwKD7Zm_MjqW","timestamp":1622106936931},{"file_id":"1QblOtm62z9o1JIL5BVVghxIfnaxJ-Xtw","timestamp":1622099766973},{"file_id":"1gcmGZCqZ_dyjzG5wG6b5xgRBEGH0BKdK","timestamp":1621966104442},{"file_id":"10dyVnpAJogKpKJo-l5KYpqNYLxR8v0v8","timestamp":1621951584071},{"file_id":"1i_LnVOfxnPfx3KpC89VtMAf6M6eKIip1","timestamp":1621802010244},{"file_id":"1E4TbDqseXZ6wxtPDbKKuKUpB7F076PP4","timestamp":1621784336493}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
